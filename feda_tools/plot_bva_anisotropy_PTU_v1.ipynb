{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97d24566-8c72-441f-b799-dbfcdece36b1",
   "metadata": {},
   "source": [
    "# Compute BVA histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae83a0e3-2251-4f81-8659-4fae5c3fce43",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2366c225-faf1-4e5b-8472-925a2a729a96",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import tttrlib\n",
    "\n",
    "import os\n",
    "\n",
    "#this is a test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ad922-d3cb-4553-9b23-d533e85ecff4",
   "metadata": {},
   "source": [
    "## Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "916ee048-e509-4b96-8d7c-1eafcbfe7d51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def update_tttr_dict(\n",
    "    df: pd.DataFrame,\n",
    "    data_path: pathlib.Path,\n",
    "    tttrs: Dict[str, tttrlib.TTTR] = dict()    \n",
    "):\n",
    "    for ff, fl in zip(df['First File'], df['Last File']):\n",
    "        try:\n",
    "            tttr = tttrs[ff]\n",
    "        except KeyError:\n",
    "            fn = str(data_path / ff)\n",
    "            #tttr = tttrlib.TTTR(fn, 'HT3')\n",
    "            tttr = tttrlib.TTTR(fn, 'PTU')\n",
    "            #tttr = tttrlib.TTTR(fn, 'SPC-130')\n",
    "            tttrs[ff] = tttr    \n",
    "    return tttrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b386420-37b0-4996-9863-5ddb0b104a7d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def read_analysis(\n",
    "    paris_path : pathlib.Path,\n",
    "    paths: List[str] = ['bg4', 'bi4_bur'], #\n",
    "    file_endings: List[str] = ['bg4', 'bur']  # \n",
    ") -> (pd.DataFrame, Dict[str, tttrlib.TTTR]):\n",
    "    \n",
    "    info_path = paris_path / 'Info'\n",
    "    data_path = paris_path.parent\n",
    "\n",
    "    dfs = list()\n",
    "    for path, ending in zip(paths, file_endings):\n",
    "        frames = list()\n",
    "        for fn in sorted((paris_path / path).glob('*.%s' % ending)):\n",
    "            with open(fn) as f:\n",
    "                t = f.readlines()\n",
    "                #if structure added to accommodate bur files which do not have '\\n' col.\n",
    "                if ending == 'bur':\n",
    "                    h = t[0].split('\\t')\n",
    "                else:\n",
    "                    h = t[0].split('\\t')[:-1]\n",
    "                #print(str(len(h)) + path) 'for debugging'\n",
    "                d = [[x for x in l.split('\\t')] for l in t[2::2]]\n",
    "                frames.append(pd.DataFrame(d, columns=h))\n",
    "        path_df = pd.concat(frames)\n",
    "        dfs.append(path_df)\n",
    "    df = pd.concat(dfs, axis=1)\n",
    "    \n",
    "#     df = df.dropna()\n",
    "    \n",
    "    # Loop through each column and attempt to convert to numeric\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            df[column] = pd.to_numeric(df[column])\n",
    "        except ValueError:\n",
    "            # Handle exceptions, e.g., if the column contains non-numeric values\n",
    "            print(f\"Could not convert {column} to numeric\")\n",
    "    tttrs = dict()\n",
    "    update_tttr_dict(df, data_path, tttrs)\n",
    "    return df, tttrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4aef8bb4-cf54-44ba-8331-75c0b773d470",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_bva_anisotropy_mean_std(\n",
    "    df: pd.DataFrame, # Target data frame\n",
    "    # Green parallel\n",
    "    green_p_channels: List[int] =[0],  \n",
    "    green_p_micro_time_ranges: List[Tuple[int, int]] = [(0, 12499)], # First & Last Channel Number (Prompt) PQ HydraHarp (TAC Channels)\n",
    "    \n",
    "    # Green Perpendicular\n",
    "    green_s_channels: List[int] = [2],\n",
    "    green_s_micro_time_ranges: List[Tuple[int, int]] = [(0, 12499)], # First & Last Channel Number (Prompt) PQ HydraHarp (TAC Channels)\n",
    "    \n",
    "    #time/photon windown\n",
    "    minimum_window_length: float = 0.01,  # size of time window within burst\n",
    "    number_of_photons_per_slice: int = -1\n",
    "):\n",
    "    # Iterate through rows using iterrows()\n",
    "    rs_ratios_mean, rs_ratios_sd = list(), list() # r_scatter\n",
    "    mt_green_p_mean, mt_green_p_sd = list(), list() # Parallel\n",
    "    mt_green_s_mean, mt_green_s_sd = list(), list() # Perpendicular\n",
    "    \n",
    "    for index, row in tqdm.tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        \n",
    "        # Select tttr data of burst out of dictionary\n",
    "        ff, fl = row['First File'], row['Last File']\n",
    "        tttr = tttrs[ff]\n",
    "        time_calibration = tttr.header.tag('MeasDesc_GlobalResolution')['value'] # For PTU Files\n",
    "        #time_calibration = 1/(tttr.header.tag('SyncRate')['value']) # For HT3 files\n",
    "        #time_calibration = 5.00E-8\n",
    "        #print(time_calibration) # 50 ns 5E-8\n",
    "        \n",
    "        # Select events within burst\n",
    "        burst_start, burst_stop = int(row['First Photon']), int(row['Last Photon'])\n",
    "        burst_tttr = tttr[burst_start:burst_stop]\n",
    "        \n",
    "        # Either select TWs by the number of photons in a TW or by the number of photons\n",
    "        if number_of_photons_per_slice < 0:\n",
    "            \n",
    "            # Split burst into time windows\n",
    "            burst_tws = burst_tttr.get_ranges_by_time_window(minimum_window_length, macro_time_calibration=time_calibration)\n",
    "            burst_tws = burst_tws.reshape((len(burst_tws)//2, 2))\n",
    "            \n",
    "            # Create tttr objects of burst TWs\n",
    "            burst_tws_tttr = [burst_tttr[start: stop] for start, stop in burst_tws]\n",
    "        else:\n",
    "            # Split bursts into chunks with a certain number of photons\n",
    "            chunk_size = number_of_photons_per_slice\n",
    "            burst_tws_tttr = [burst_tttr[i:i+chunk_size] for i in range(0, len(burst_tttr), chunk_size)]\n",
    "    \n",
    "        # Compute for each time window in a burst the proximity ratio\n",
    "        n_green_p, n_green_s = list(), list()\n",
    "        mt_green_p, mt_green_s = list(), list()\n",
    "        for tw_tttr in burst_tws_tttr:            \n",
    "            # mask all photons that are not Green photons (Parallel)\n",
    "            mask_green_p = tttrlib.TTTRMask()\n",
    "            mask_green_p.select_channels(tw_tttr, green_p_channels)\n",
    "            mask_green_p.select_microtime_ranges(tw_tttr, green_p_micro_time_ranges)\n",
    "            \n",
    "            # mask all photons that are not Green photons (Perpendicular)\n",
    "            mask_green_s = tttrlib.TTTRMask()\n",
    "            mask_green_s.select_channels(tw_tttr, green_s_channels)\n",
    "            mask_green_s.select_microtime_ranges(tw_tttr, green_s_micro_time_ranges)\n",
    "            \n",
    "            # Assign time window to selector\n",
    "            mask_green_p.set_tttr(tw_tttr)\n",
    "            mask_green_s.set_tttr(tw_tttr)\n",
    "            \n",
    "            # Get number of green parallel and perpendicular photons in TW\n",
    "            n_green_p.append(len(mask_green_p.get_indices()))\n",
    "            n_green_s.append(len(mask_green_s.get_indices()))\n",
    "\n",
    "            # Get the mean micro time\n",
    "            mt_green_p.append(tw_tttr[mask_green_p.get_indices()].get_mean_microtime())\n",
    "            mt_green_s.append(tw_tttr[mask_green_s.get_indices()].get_mean_microtime())\n",
    "\n",
    "        # Compute mean and sd of intensities rs (Anisotropy)\n",
    "        tw_n_green_p = np.array(n_green_p)\n",
    "        tw_n_green_s = np.array(n_green_s)\n",
    "        \n",
    "        #Correction Fractors G-factor and background\n",
    "        G = 1/g\n",
    "        duration = duration_tw/2 # Calcualtion rom Time windows duration_tw = 5/mean_cr. Divided into 2 because prompt and delay\n",
    "        tw_B_p = B_p*duration\n",
    "        tw_B_s = B_s*duration\n",
    "        \n",
    "        #Calculation\n",
    "        tw_rs_ratios = (tw_n_green_p - tw_B_p - G*(tw_n_green_s-tw_B_s))/ (tw_n_green_p - tw_B_p + 2*G*(tw_n_green_s - tw_B_s))\n",
    "        rs_ratio_mean, rs_ratio_sd = tw_rs_ratios.mean(), tw_rs_ratios.std()\n",
    "        rs_ratios_mean.append(rs_ratio_mean)\n",
    "        rs_ratios_sd.append(rs_ratio_sd)\n",
    "\n",
    "        # Compute mean and sd of microtimes Green Parallel and Perpendicular\n",
    "        mt_green_p = np.array(mt_green_p)\n",
    "        mt_green_p_mean.append(mt_green_p.mean())\n",
    "        mt_green_p_sd.append(mt_green_p.std())\n",
    "\n",
    "        mt_green_s = np.array(mt_green_s)\n",
    "        mt_green_s_mean.append(mt_green_s.mean())\n",
    "        mt_green_p_sd.append(mt_green_s.std())\n",
    "\n",
    "    rs_ratios_mean = np.array(rs_ratios_mean)\n",
    "    rs_ratios_sd = np.array(rs_ratios_sd)\n",
    "    df['Anisotropy Mean'] = rs_ratios_mean #rscatter (background corrected)\n",
    "    df['Anisotropy Std'] = rs_ratios_sd #rscatter (background corrected)\n",
    "    \n",
    "    #df['Microtime Green P Mean'] = mt_green_p_mean\n",
    "    #df['Microtime Green P Std'] = mt_green_p_sd\n",
    "\n",
    "    #df['Microtime Green S Mean'] = mt_green_s_mean\n",
    "    #df['Microtime Green S Std'] = mt_green_s_sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8811c5cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def compute_static_simulated_species(\n",
    "    number_of_photons_per_slice_sim: int = 4\n",
    ") -> Tuple[np.array, np.array]:\n",
    "    prox_mean = list()\n",
    "    prox_sd = list()\n",
    "    n_samples = 10000\n",
    "    for prox in prox_mean_bins:\n",
    "        prox_sim_red = np.random.binomial(number_of_photons_per_slice_sim, prox, n_samples)\n",
    "        prox_sim_green = number_of_photons_per_slice_sim - prox_sim_red\n",
    "        prox = prox_sim_red / (prox_sim_red + prox_sim_green)\n",
    "        prox_mean.append(prox.mean())\n",
    "        prox_sd.append(prox.std())\n",
    "    return np.array(prox_mean), np.array(prox_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "560e31c0-dbd6-45dd-bd15-8d69e3ae0f6a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def average_histogram(\n",
    "    counts: np.ndarray,\n",
    "    x_bins: np.ndarray,\n",
    "    y_bins: np.ndarray\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute the mean and the sd over the bins of a 2D histogram\n",
    "    \"\"\"\n",
    "    mean = list()\n",
    "    sd = list()\n",
    "    h = counts\n",
    "    y = y_bins[1:]\n",
    "    y2 = y * y\n",
    "    x = x_bins[1:]\n",
    "    for i, prox in enumerate(x):\n",
    "        c = h[i]\n",
    "        s = np.sum(c)\n",
    "        m1 = c @ y / s\n",
    "        m2 = c @ y2 / s\n",
    "        mean.append(m1)\n",
    "        v = np.sqrt(m2 - m1*m1) / np.sqrt(s)\n",
    "        sd.append(v)\n",
    "    sd = np.array(sd)\n",
    "    mean = np.array(mean)\n",
    "    return mean, sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c81c4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_burstids(df, bid_path):\n",
    "    \n",
    "    # get bid_df to use as a filter on the passed df.\n",
    "    bid_df = get_bid_df(bid_path)\n",
    "    \n",
    "    # Filter df using flphotons.\n",
    "    result_df = df.merge(bid_df, on=['First Photon', 'Last Photon', 'First File'], how='inner')\n",
    "\n",
    "#     Debug\n",
    "#     bid_df.to_csv(\"bid_df.csv\", index=False)\n",
    "#     result_df.to_csv(\"result_df.csv\", index=False)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65650261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all of the burst id files in the selected directory and create a dataframe storing \n",
    "# their First Photon, Last Photon, and First File values row-wise.\n",
    "\n",
    "def get_bid_df(bid_path):\n",
    "\n",
    "    # create dfs list to append each bst file dataframe when reading the directory\n",
    "    dfs = []\n",
    "    \n",
    "    # iterate through the bst files in the specified directory\n",
    "    for file in sorted((bid_path).glob('*.%s' % 'bst')):\n",
    "        \n",
    "        # read each file and create a dataframe\n",
    "        file_df = pd.read_csv(file, sep ='\\t', header=None)\n",
    "        \n",
    "        # rename the unnammed columns\n",
    "        file_df.rename(columns={0: \"First Photon\", 1: \"Last Photon\"}, inplace=True)\n",
    "        \n",
    "        # get the filename so we can assoc a file to the First File column\n",
    "        filename = os.path.basename(file).split('.')[0]\n",
    "        \n",
    "        # process the filename so that it matches the format in the bur file\n",
    "        filename = filename.replace(\"_0\", \"\")\n",
    "        filename = filename + \".ptu\"\n",
    "\n",
    "        # assign this files First and Last Photon data to the associated file.\n",
    "        file_df['First File'] = filename\n",
    "        \n",
    "        # append the burst file dataframe to a list, will concatenate them all after loop.\n",
    "        dfs.append(file_df)\n",
    "        \n",
    "    # concatenate all the dfs into one \n",
    "    flphotons = pd.concat(dfs, axis = 0, ignore_index = True)\n",
    "\n",
    "    return flphotons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417b79f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot(x, y, xlabel, ylabel, nx, ny):\n",
    "\n",
    "    n_binsx = nx\n",
    "    n_binsy = ny\n",
    "    c_map = 'gist_ncar_r'\n",
    "\n",
    "    # Create a Figure, which doesn't have to be square.\n",
    "    fig = plt.figure(layout='constrained')\n",
    "    \n",
    "    # Create the main axes, leaving 25% of the figure space at the top and on the\n",
    "    # right to position marginals.\n",
    "    ax = fig.add_gridspec(top=0.75, right=0.75).subplots()\n",
    "    \n",
    "    # The main axes' aspect can be fixed.\n",
    "    ax.set(aspect=\"auto\")\n",
    "    ax_histx = ax.inset_axes([0, 1.05, 1.0, 0.25], sharex=ax)\n",
    "    ax_histy = ax.inset_axes([1.05, 0, 0.25, 1], sharey=ax)\n",
    "    # ax_cbar = ax.inset_axes([1.05, 1.05, 0.25, .25])\n",
    "    # ax_histy = ax.inset_axes([1.05, .5, 0.25, 1], sharey=ax)\n",
    "    # ax_cbar = ax.inset_axes([1.05, 0, 0.25, 1], sharey=ax)\n",
    "    # Draw the scatter plot and marginals.\n",
    "    # no labels\n",
    "    ax_histx.tick_params(axis=\"x\", labelbottom=False)\n",
    "    ax_histy.tick_params(axis=\"y\", labelleft=False)\n",
    "\n",
    "    if ylabel == \"Sg/Sr (prompt)\":\n",
    "        n_binsy = np.geomspace(np.min(y), np.max(y), num=n_binsy)\n",
    "        plt.yscale(\"log\")\n",
    "\n",
    "    # the 2d hist plot:\n",
    "    h = ax.hist2d(x, y, bins = [n_binsx, n_binsy], range=[[xmin, xmax], [ymin, ymax]], \n",
    "                  cmap = c_map)\n",
    "    mappable = h[3]\n",
    "    fig.colorbar(mappable, ax=ax, location='left')\n",
    "\n",
    "\n",
    "    # now determine nice limits by hand:\n",
    "    binwidth = 0.25\n",
    "    xymax = max(np.max(np.abs(x)), np.max(np.abs(y)))\n",
    "    lim = (int(xymax/binwidth) + 1) * binwidth\n",
    "\n",
    "    bins = np.arange(-lim, lim + binwidth, binwidth)\n",
    "    \n",
    "    nx = ax_histx.hist(x, bins=n_binsx, range=[xmin, xmax])\n",
    "    ny = ax_histy.hist(y, bins=n_binsy, range=[ymin, ymax], orientation='horizontal')\n",
    "    #nx = ax_histx.hist(x, bins=n_binsx)\n",
    "    #ny = ax_histy.hist(y, bins=n_binsy, orientation='horizontal')\n",
    "\n",
    "    ax.set_xlabel(xlabel, fontsize = 20)\n",
    "    ax.set_ylabel(ylabel, fontsize = 20)\n",
    "    \n",
    "    return nx, ny"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21f787c-f54a-4dd3-b4a4-d09f0fcfe42a",
   "metadata": {},
   "source": [
    "## Read single molecule analysis & Correction Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95ccae61-0bd9-4f3c-82b3-82133ee5bf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PTU Files\n",
    "file_path = pathlib.Path('/Users/hugosanabria/Dropbox/MFD data 2020/20220314_FoxP1_data_all_NK/20220312_V78C_dimer-DNA_NK/V78C_dimer-DNA_FoxP1_1hr/burstwise_All 0.1771#30')\n",
    "bid_path = pathlib.Path('/Users/hugosanabria/Dropbox/MFD data 2020/20220314_FoxP1_data_all_NK/20220312_V78C_dimer-DNA_NK/V78C_dimer-DNA_FoxP1_1hr/burstwise_All 0.1771#30/BIDs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "104e2bbf-a32b-4fbb-8a42-ec65e6a78f7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "13 columns passed, passed data had 12 columns",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/tttrlib_env/lib/python3.8/site-packages/pandas/core/internals/construction.py:934\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_or_indexify_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tttrlib_env/lib/python3.8/site-packages/pandas/core/internals/construction.py:981\u001b[0m, in \u001b[0;36m_validate_or_indexify_columns\u001b[0;34m(content, columns)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_mi_list \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(columns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(content):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    980\u001b[0m     \u001b[38;5;66;03m# caller's responsibility to check for this...\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    982\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns passed, passed data had \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    983\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(content)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m columns\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    984\u001b[0m     )\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_mi_list:\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 13 columns passed, passed data had 12 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_path \u001b[38;5;241m=\u001b[39m file_path\n\u001b[0;32m----> 2\u001b[0m df, tttrs \u001b[38;5;241m=\u001b[39m \u001b[43mread_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m, in \u001b[0;36mread_analysis\u001b[0;34m(paris_path, paths, file_endings)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m#print(str(len(h)) + path) 'for debugging'\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         d \u001b[38;5;241m=\u001b[39m [[x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m l\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m)] \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m t[\u001b[38;5;241m2\u001b[39m::\u001b[38;5;241m2\u001b[39m]]\n\u001b[0;32m---> 23\u001b[0m         frames\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m path_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(frames)\n\u001b[1;32m     25\u001b[0m dfs\u001b[38;5;241m.\u001b[39mappend(path_df)\n",
      "File \u001b[0;32m~/anaconda3/envs/tttrlib_env/lib/python3.8/site-packages/pandas/core/frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    781\u001b[0m         columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[0;32m--> 782\u001b[0m     arrays, columns, index \u001b[38;5;241m=\u001b[39m \u001b[43mnested_data_to_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[39;49;00m\n\u001b[1;32m    784\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[39;49;00m\n\u001b[1;32m    785\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    790\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    791\u001b[0m         arrays,\n\u001b[1;32m    792\u001b[0m         columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    795\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    796\u001b[0m     )\n\u001b[1;32m    797\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tttrlib_env/lib/python3.8/site-packages/pandas/core/internals/construction.py:498\u001b[0m, in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_named_tuple(data[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    496\u001b[0m     columns \u001b[38;5;241m=\u001b[39m ensure_index(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_fields)\n\u001b[0;32m--> 498\u001b[0m arrays, columns \u001b[38;5;241m=\u001b[39m \u001b[43mto_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m columns \u001b[38;5;241m=\u001b[39m ensure_index(columns)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tttrlib_env/lib/python3.8/site-packages/pandas/core/internals/construction.py:840\u001b[0m, in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    837\u001b[0m     data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtuple\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[1;32m    838\u001b[0m     arr \u001b[38;5;241m=\u001b[39m _list_to_arrays(data)\n\u001b[0;32m--> 840\u001b[0m content, columns \u001b[38;5;241m=\u001b[39m \u001b[43m_finalize_columns_and_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m content, columns\n",
      "File \u001b[0;32m~/anaconda3/envs/tttrlib_env/lib/python3.8/site-packages/pandas/core/internals/construction.py:937\u001b[0m, in \u001b[0;36m_finalize_columns_and_data\u001b[0;34m(content, columns, dtype)\u001b[0m\n\u001b[1;32m    934\u001b[0m     columns \u001b[38;5;241m=\u001b[39m _validate_or_indexify_columns(contents, columns)\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    936\u001b[0m     \u001b[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m    939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(contents) \u001b[38;5;129;01mand\u001b[39;00m contents[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mobject_:\n\u001b[1;32m    940\u001b[0m     contents \u001b[38;5;241m=\u001b[39m convert_object_array(contents, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: 13 columns passed, passed data had 12 columns"
     ]
    }
   ],
   "source": [
    "data_path = file_path\n",
    "df, tttrs = read_analysis(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0cc252-ea51-46db-9344-883ac9f44434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the full data set using selected burst ids\n",
    "df = filter_burstids(df, bid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a8b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Column Names\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3553c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Correction g-factor \n",
    "g = 1.0 #\n",
    "B_p = 1.6 # Background kHz Green p\n",
    "B_s = 1.1 # Background kHz Green s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd19b90a",
   "metadata": {},
   "source": [
    "## Calculate MFD Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756d3764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Sg/Sr (prompt)\n",
    "S_G = df['Green Count Rate (KHz)'] #df['Number of Photons (fit window) (green)']/df['Duration (green) (ms)'] \n",
    "S_R = df['S prompt red (kHz) | 0-180']\n",
    "\n",
    "df['Sg/Sr (prompt)_Burst'] = S_G/S_R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad0df6e",
   "metadata": {},
   "source": [
    "# Plot Various Burtwise MFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0d373a-4805-43b5-92e0-de91e123e8e6",
   "metadata": {},
   "source": [
    "#### Histogram over burst durations, Count Rate, Number of Photons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a65f42-7f56-4325-9441-8943fa22f7dc",
   "metadata": {},
   "source": [
    "Display the burst duration and determine the time window for the based on the mode (the most frequent burst duration) the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9650d9-edc6-4c6a-a3ce-4910dfc59cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Bust duration')\n",
    "counts_bd, bins_bd, patches_bd =  plt.hist(df['Duration (ms)'], bins=np.linspace(0, 40, 41), log=False, rwidth=0.6, linewidth=1., edgecolor='black', fc=(0.1, 0.1, 0.1, 0.2))\n",
    "plt.xlabel('Duration (ms)')\n",
    "plt.ylabel('Counts')\n",
    "# Mean Duration\n",
    "number_of_tws_in_mode = 3\n",
    "mean_dur = counts_bd @ bins_bd[1:] / np.sum(counts_bd)\n",
    "minimum_window_length = (mean_dur / 1000.0) / number_of_tws_in_mode\n",
    "print(\"Mean Duration {:.1f} ms\".format(mean_dur))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b93dcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tau (green)'] = df['Tau (green)'].astype(float) ## Convert To Float This columns did not convert co numeric see warning above\n",
    "plt.title('Tau_G Burstwise')\n",
    "counts_TG, bins_TG, patches_TG =  plt.hist(df['Tau (green)'], bins=np.linspace(0, 8, 41), log=False, rwidth=0.6, linewidth=1., edgecolor='black', fc=(0.1, 0.1, 0.1, 0.2))\n",
    "plt.xlabel('Tau (green)')\n",
    "plt.ylabel('Counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['r Scatter (green)'] = df['r Scatter (green)'].astype(float) ## Convert To Float This columns did not convert co numeric see warning above\n",
    "\n",
    "\n",
    "[xmin, xmax], [ymin, ymax] = (0, 8), (-0.2, 0.6)\n",
    "bins = 41, 41\n",
    "vmin, vmax = 0.1, 41\n",
    "nx, ny = make_plot(df['Tau (green)'], df['r Scatter (green)'], \n",
    "          'Lifetime (ns)', 'Anisotropy', 41, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3afdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Sg/Sr (prompt)')\n",
    "counts_SgSr, bins_SgSr, patches_SgSr =  plt.hist(df['Sg/Sr (prompt)_Burst'], bins=np.logspace(np.log10(0.01), np.log10(185), 41), log=False, rwidth=0.6, linewidth=1., edgecolor='black', fc=(0.1, 0.1, 0.1, 0.2))\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Sg/Sr (prompt)')\n",
    "plt.ylabel('Events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2758cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Count Rate')\n",
    "counts_cr, bins_cr, patches_cr =  plt.hist(df['Count Rate (KHz)'], bins=np.linspace(0, 100, 41), log=False, rwidth=0.6, linewidth=1., edgecolor='black', fc=(0.1, 0.1, 0.1, 0.2))\n",
    "plt.xlabel('Count Rate (KHz)')\n",
    "plt.ylabel('Events')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184af99",
   "metadata": {},
   "source": [
    "## Calculate mean burst durations, Count Rate, Number of Photons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6347340-6b04-49d0-ad43-925f7896f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Duration\n",
    "number_of_tws_in_mode = 3\n",
    "mean_dur = counts_bd @ bins_bd[1:] / np.sum(counts_bd)\n",
    "minimum_window_length = (mean_dur / 1000.0) / number_of_tws_in_mode\n",
    "print(\"Mean Duration {:.1f} ms\".format(mean_dur))\n",
    "\n",
    "# Mean Count Rate\n",
    "number_of_tws_in_mode = 3\n",
    "mean_cr = counts_cr @ bins_cr[1:] / np.sum(counts_cr)\n",
    "print(\"Mean Count Rate {:.1f} KHz\".format(mean_cr))\n",
    "\n",
    "# Mean Time for # Number of Photons\n",
    "number_of_photons_per_slice = 30 # Change this every run\n",
    "duration_tw = number_of_photons_per_slice/mean_cr\n",
    "print(\"Duration Time Window {:.3f}\".format(duration_tw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6655aa66-be10-4c5e-97fe-226228774202",
   "metadata": {},
   "source": [
    "# Compute mean and variance in bursts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbcddf-3137-418d-b366-cf42addc7895",
   "metadata": {},
   "source": [
    "### BVA over fixed number of photons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e493fd71-a51b-4fee-8907-97196bc629f5",
   "metadata": {},
   "source": [
    "### Compute mean Anisotropy and variance for simulated static species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cb254",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_of_photons_per_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2ab63",
   "metadata": {},
   "source": [
    "### Plot 2D BVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f40602",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "###       Compute BVA Anisotropy        ###\n",
    "###########################################\n",
    "compute_bva_anisotropy_mean_std(df, number_of_photons_per_slice=number_of_photons_per_slice)\n",
    "#df_selected = df[df['Anisotropy Std'] > 0.0]\n",
    "[xmin, xmax], [ymin, ymax] = (-0.2, 0.6), (0, 1)\n",
    "vmin, vmax = 0.1, 30\n",
    "counts, rs_mean_bins, rs_sd_bins, _ = plt.hist2d(df['Anisotropy Mean'], df['Anisotropy Std']**2, range=[[xmin, xmax], [ymin, ymax]], bins=(41, 41), vmin=vmin, vmax=vmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4732dcc",
   "metadata": {},
   "source": [
    "### Make Final Plot and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97953b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "[xmin, xmax], [ymin, ymax] = (-0.2, 0.6), (0, 1)\n",
    "bins = 41, 41\n",
    "vmin, vmax = 0.1, 30\n",
    "x = df['Anisotropy Mean']\n",
    "#y = df['Anisotropy Std']**2 # All variance\n",
    "y = df['Anisotropy Std']**2-(df['Anisotropy Mean']*(1-df['Anisotropy Mean'])/number_of_photons_per_slice) # Excess Varaiance S^2 = s^2 - sigma^2, where sigma^2 = E(1-E)/m\n",
    "nx, ny = make_plot(x, y, \n",
    "          'Anisotropy (rs)', 'Excess Variance rs, SD(rs)^2', 41, 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f368299",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save files\n",
    "countsdf = pd.DataFrame(counts)\n",
    "countsdf.to_csv(str(file_path) + '/counts_' + str(number_of_photons_per_slice) + '_photons' + '.csv')\n",
    "\n",
    "nxdf = pd.DataFrame(nx)\n",
    "nxdf.to_csv(str(file_path) + '/nx_' + str(number_of_photons_per_slice) + '_photons' + '.csv')\n",
    "\n",
    "nydf = pd.DataFrame(ny)\n",
    "nydf.to_csv(str(file_path) + '/ny_' + str(number_of_photons_per_slice) + '_photons' + '.csv')\n",
    "\n",
    "nmean_xdf = pd.DataFrame(x_mean)\n",
    "nmean_xdf.to_csv(str(file_path) + '/meanE_' + str(number_of_photons_per_slice) + '_photons' + '.csv')\n",
    "\n",
    "nmean_ydf = pd.DataFrame(y_mean)\n",
    "nmean_ydf.to_csv(str(file_path) + '/meanvar_' + str(number_of_photons_per_slice) + '_photons' +'.csv')\n",
    "\n",
    "nerr_ydf = pd.DataFrame(yerr)\n",
    "nerr_ydf.to_csv(str(file_path) + '/errvar_' + str(number_of_photons_per_slice) + '_photons'+ '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdd6f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
