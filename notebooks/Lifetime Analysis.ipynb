{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf0902c-cc14-4ae0-9dc3-0fa7c2491a8d",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3052ae27-0a40-4d08-b145-f84eb89e1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import pylab\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import tttrlib\n",
    "\n",
    "import os\n",
    "\n",
    "from feda_tools import twodim_hist as tdh\n",
    "from feda_tools import utilities as utils\n",
    "from feda_tools import analysis as an\n",
    "\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import halfnorm\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e9ad2-aec6-4ec5-908c-00ddb355f94c",
   "metadata": {},
   "source": [
    "## Declare required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dac192f1-4cf3-4b97-b9ac-9551edd81bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flour_aniso(g_factor, intensity_para, intensity_perp, l1_japan_corr, l2_japan_corr):\n",
    "    ### Fluorescence Anisotropy calculation. see equation 7 in Kudryavtsev, V., Sikor, M., Kalinin, S., Mokranjac, D., Seidel, C.A.M. and Lamb, D.C. (2012), \n",
    "    ### Combining MFD and PIE for Accurate Single-Pair FÃ¶rster Resonance Energy Transfer Measurements. ChemPhysChem, 13: 1060-1078. https://doi.org/10.1002/cphc.201100822\n",
    "    \n",
    "    return (g_factor * intensity_para - intensity_perp) / ((1 - 3 * l2_japan_corr) * g_factor * intensity_para + (2 - 3 * l1_japan_corr) * intensity_perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15b425-cdea-4f49-9141-974fccc68131",
   "metadata": {},
   "source": [
    "## Load the target PTU file\n",
    "\n",
    "-  **repo_path** is the path to the feda_tools repository. It must be updated to reflect the location of the repo on the system running this notebook before continuing with the analysis. <br>\n",
    "-  **dir** is the directory in which the target PTU file is located. <br>\n",
    "- **file_ptu** is the target PTU file on which the analysis will be performed. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56416358-d044-42cf-acff-e49b575d19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PTU Files\n",
    "# file_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30')\n",
    "# bid_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30/BIDs_30ph')\n",
    "\n",
    "### for testing purposes ###\n",
    "# file_path = pathlib.Path('C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu')\n",
    "\n",
    "# #total time 816.9 seconds for this file\n",
    "# dir = 'C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/'\n",
    "# file_ptu = dir + 'Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu'\n",
    "# file_irf = dir + 'H2O_300s_adjust_thresholds.ptu'\n",
    "# file_bkg = dir + 'PBS_300s_adjust_thresholds.ptu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d38e020-b770-49c7-8247-dbe675998f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PTU Files\n",
    "# file_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30')\n",
    "# bid_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30/BIDs_30ph')\n",
    "\n",
    "### for testing purposes ###\n",
    "# file_path = pathlib.Path('C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu')\n",
    "\n",
    "#total time 816.9 seconds for this file\n",
    "dir = '../test data/2024/'\n",
    "file_ptu = dir + 'Split_20230809_HighFRETDNAStd_1hr_Dani-000000.ptu'\n",
    "# file_irf = dir + 'H2O_300s_adjust_thresholds.ptu'\n",
    "file_irf = dir + '20230809_IRFddH2O_Dani_5min.ptu'\n",
    "# file_bkg = dir + 'PBS_300s_adjust_thresholds.ptu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c37af5-7223-4a4f-b10e-0eacfb495a03",
   "metadata": {},
   "source": [
    "## Initialize tttrlib data and extract important global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bf7205d-e2f0-42bd-92a1-916a43bf2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define analysis window for subset of PTU\n",
    "min_event = 0\n",
    "max_event = 300000\n",
    "\n",
    "data_ptu = tttrlib.TTTR(file_ptu, 'PTU')\n",
    "routing_channels =  data_ptu.routing_channels\n",
    "\n",
    "# total duration in seconds\n",
    "all_macro_times = data_ptu.macro_times\n",
    "all_micro_times = data_ptu.micro_times\n",
    "micro_res = data_ptu.get_header().micro_time_resolution\n",
    "macro_res =data_ptu.get_header().macro_time_resolution\n",
    "total_duration = all_macro_times[-1] * macro_res\n",
    "\n",
    "data_irf = tttrlib.TTTR(file_irf, 'PTU')\n",
    "all_macro_times_irf = data_irf.macro_times\n",
    "all_micro_times_irf = data_irf.micro_times\n",
    "routing_channels_irf = data_irf.routing_channels\n",
    "\n",
    "# data_bkg = tttrlib.TTTR(file_bkg, 'PTU')\n",
    "# all_macro_times_bkg = data_bkg.macro_times\n",
    "# all_micro_times_bkg = data_bkg.micro_times\n",
    "# routing_channels_bkg =  data_bkg.routing_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe224-8caf-46a3-9a0d-532345adaab1",
   "metadata": {},
   "source": [
    "## Determine analysis settings for bur, bg4, by4, and br4 calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "681c0cfe-13f2-4e14-9e37-a3183b60673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# photon count threshold for burst selection\n",
    "min_photon_count = 60\n",
    "\n",
    "# this window changes for br4 and by4\n",
    "# bg4 parameters\n",
    "bg4_micro_time_min = 0\n",
    "bg4_micro_time_max = 12499\n",
    "\n",
    "# flourescence anisotropy parameters\n",
    "g_factor = 0.4\n",
    "l1_japan_corr = 0.0308\n",
    "l2_japan_corr = 0.0368\n",
    "\n",
    "# # bkg signals required for r Scatter calculations\n",
    "bg4_bkg_para = 0\n",
    "bg4_bkg_perp = 0\n",
    "\n",
    "\n",
    "# MLE parameters\n",
    "num_bins = 128\n",
    "bin_width = macro_res/micro_res/num_bins/1000 # in nanoseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b2f2-15a8-4f05-9bd5-000d6edeaf5b",
   "metadata": {},
   "source": [
    "## Burst Selection: Calculate Interphoton Arrival Time \n",
    "\n",
    "- Each detected photon has a time of detection encoded by the macro time + the micro time. **all_macro_times** and **all_micro_times** are arrays whose index is represents the detected photons in order of detection, while the value represents the associated macro or micro time for each photon.\n",
    "- **macro_res** and **micro_res** represent the resolution of the macro and micro times in seconds.\n",
    "- The **macro time** indicates the time in units of **macro_res** that the excitation laser was last fired directly before this photon was detected.\n",
    "- The **micro time** indicates the amount of time in units of **micro_res** that has elapsed since the excitation laser was last fired at which the photon was detected, i.e. it's the amount of time elapsed from the macro time at which the photon was detected.\n",
    "- The interphoton arrival time is calculated by iterating through **all_macro_times** and **all_micro_times** and calculating the time elapsed between each photon detection event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "830cd919-50e6-4812-80cc-618ebbd8b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "photon_time_intervals = an.calc_interphoton_arrival_times(data_ptu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20dfecc0-5424-4b37-ae8e-2570935704a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14390862645541347\n"
     ]
    }
   ],
   "source": [
    "print(photon_time_intervals[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0016ec6-6c36-498d-a6b4-a2e2f1410c95",
   "metadata": {},
   "source": [
    "## Burst Selection: Calculate the Log of the Running Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a02b49a-8a02-49a7-b0f7-84055deb7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the window size for the running average\n",
    "window_size = 30\n",
    "\n",
    "# Calculate the running average\n",
    "running_avg = an.calc_running_average(photon_time_intervals, window_size)\n",
    "\n",
    "# Create x axis array to match the data.\n",
    "xarr = np.arange(window_size - 1, len(photon_time_intervals))\n",
    "\n",
    "# Calculate the base 10 log of the running average\n",
    "logrunavg = np.log10(running_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a3ff9-882b-4153-b8b1-90f035c2113f",
   "metadata": {},
   "source": [
    "## Burst Selection: Visualize the Photon Events with an Interactive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6836af45-ca6c-4d31-b625-06212e102805",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m xrange \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m : min_event, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m : max_event}\n\u001b[1;32m      5\u001b[0m yrange \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m : \u001b[38;5;241m2\u001b[39m}\n\u001b[0;32m----> 6\u001b[0m fig, ax, twodimdata \u001b[38;5;241m=\u001b[39m tdh\u001b[38;5;241m.\u001b[39mmake_plot(xarr, logrunavg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m,xrange ,yrange, bins)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# plot the running average as a 2D histogram with 1D histograms on the margins\n",
    "%matplotlib qt\n",
    "bins = {\"x\":141, \"y\": 141}\n",
    "xrange = {\"min\" : min_event, \"max\" : max_event}\n",
    "yrange = {\"min\" : -6, \"max\" : 2}\n",
    "fig, ax, twodimdata = tdh.make_plot(xarr, logrunavg, \"x\", \"y\",xrange ,yrange, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcc7ee-060e-4ef7-b9bd-b3fdfd77e4d6",
   "metadata": {},
   "source": [
    "## Burst Selection: Estimate the Mean of the Gaussian Background Noise \n",
    "\n",
    "\n",
    "Vary the mean value and plot to check if the estimated mean is well aligned with the peak of the noise. The data on the right-half (blue/purple) estimates the right half of the Gaussian noise. The left-most bin of the right-half data is the estimated mean. When the estimated mean is well aligned with the peak, then you may continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43ee0bf9-aeec-493d-80a5-7cae1bd24a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-17 14:01:36.570 python[72716:11021201] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-09-17 14:01:36.570 python[72716:11021201] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "# Bin the log of the running avg interphoton arrival times to find the overall data profile.\n",
    "counts_logrunavg, bins_logrunavg = np.histogram(logrunavg, bins = bins['y'])\n",
    "\n",
    "# Find the index of the maximum counts value.\n",
    "index_of_max = np.argmax(counts_logrunavg)\n",
    "\n",
    "# Use the index of the max counts to find the corresponding interphoton time bin.\n",
    "mean_est = bins_logrunavg[index_of_max]\n",
    "\n",
    "# Compress the filtered data to remove the masked values for plotting\n",
    "filtered_logrunavg = ma.masked_less(logrunavg, mean_est).compressed()\n",
    "\n",
    "# Plot to inspect the result\n",
    "counts_logrunavg, bins_logrunavg, _ = plt.hist(logrunavg, bins = bins['y'], alpha=0.6, color='r')\n",
    "plt.hist(filtered_logrunavg, bins = bins_logrunavg, alpha=0.6, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b173206f-be3a-4cc9-8035-9964df3d1f8d",
   "metadata": {},
   "source": [
    "## Burst Selection: Fit a Half-Norm to the Right Tail of the Data and Extract the Std. Dev."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afeac049-a791-4845-b63b-394cdbdd0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with halfnorm. visualize for best fit testing. get mu and std dev. consider finding max and setting location as mean\n",
    "mu, std = halfnorm.fit(filtered_logrunavg)\n",
    "\n",
    "# counts_logrunavg, bins_logrunavg, _ = plt.hist(logrunavg, bins = bins['y'], density= True, alpha=0.6, color='r')\n",
    "plt.hist(filtered_logrunavg, bins = bins['y'], density = True, alpha=0.6, color='r')\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = halfnorm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Fit Values: {:.2f} and {:.2f}\".format(mu, std)\n",
    "plt.title(title)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbfa67-9287-4eef-933f-cf936e8dde57",
   "metadata": {},
   "source": [
    "## Burst Selection: Filter Out the Noise and Plot to Inspect\n",
    "Set the threshold to 4sigma to the left of the mean, effectively isolating the protien dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30c2ab99-bc54-4a48-9415-d958b6a41db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using std from halfnorm fit, set the threshold for filtering out noise. Then, filter out noise. Raise 10 to threshold later for burst selection\n",
    "threshold_value = mu - 4*std #raise 10 to the power of this threshold to obtain the threshold in Paris.\n",
    "filtered_values = ma.masked_greater(logrunavg, threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb557882-5c02-4f69-a58f-1a4a98742716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the log running average and the threshold values\n",
    "plt.plot(xarr, logrunavg, label='Running Average', linestyle='None', marker = 'o', markersize = 5)\n",
    "plt.plot(xarr, filtered_values, label='Threshold Values', linestyle='None', marker = '.', markersize = 5)\n",
    "plt.xlabel('Photon Event #')\n",
    "plt.ylabel('log(Photon Interval Time)')\n",
    "plt.legend()\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09a742-61c9-41be-b280-e90ab34602ec",
   "metadata": {},
   "source": [
    "## Burst Selection: Create the Burst Index By Filtering with the Threshold.\n",
    "- The **burst_index** will contain a burst at each index. A burst is a collection of photon events that we hope contains protien dynamics information!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "87a38c84-e5dc-47aa-9723-0a019de722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a burst index. Each list is a burst, and each list contains the indices of \n",
    "### the photon events in the original data.\n",
    "burst_index = an.extract_unmasked_indices(filtered_values)\n",
    "\n",
    "with open( dir + 'data.pkl', 'wb') as file:\n",
    "    pickle.dump(burst_index, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4ef7377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7127, 1564129)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(burst_index), len(filtered_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4b68bb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[177, 178, 179, 180, 181, 182, 183, 184, 185]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "burst_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7f93acf-b9da-473b-9caf-8f79bb2988e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# deserialize the pickled burst index \n",
    "with open(dir + 'data.pkl', 'rb') as file:\n",
    "    burst_index = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ada07-0bca-45b7-ac8a-8ec53569ba34",
   "metadata": {},
   "source": [
    "## Burst Analysis: Create bi4_bur Dataframe and Calculate Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6abdb7c7-406a-4b23-84d3-ab6b5c09ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### create bi4_bur dataframe ###\n",
    "################################\n",
    "\n",
    "# prepare empty dataframes to insert calculated burst values\n",
    "bi4_bur_df = pd.DataFrame()\n",
    "bg4_df = pd.DataFrame()\n",
    "\n",
    "bg4_channel_2_photons_total = list()\n",
    "bg4_channel_0_photons_total = list()\n",
    "\n",
    "### calculate each burst record and store in df\n",
    "for burst in burst_index:\n",
    "\n",
    "    # filter out bursts with one or less photons.\n",
    "    if len(burst) <= min_photon_count:\n",
    "        continue\n",
    "\n",
    "    ############################# .bur calculations\n",
    "    \n",
    "    #############################\n",
    "    ### First and Last Photon ###\n",
    "    #############################\n",
    "    \n",
    "    first_photon = burst[0]\n",
    "    last_photon = burst[-1]\n",
    "\n",
    "    ##########################\n",
    "    ### Duration (ms) calc ###\n",
    "    ##########################\n",
    "    \n",
    "    lp_time = all_macro_times[last_photon]*macro_res + all_micro_times[last_photon]*micro_res\n",
    "    fp_time = all_macro_times[first_photon]*macro_res + all_micro_times[first_photon]*micro_res\n",
    "    lp_time_ms = lp_time*1000\n",
    "    fp_time_ms = fp_time*1000\n",
    "    duration = (lp_time_ms - fp_time_ms)\n",
    "\n",
    "    #################################\n",
    "    ### Mean Macro Time (ms) Calc ###\n",
    "    #################################\n",
    "    \n",
    "    # get all the macro times corresponding to the photons in this burst\n",
    "    macro_times = all_macro_times[burst[0]]*macro_res*1000\n",
    "    \n",
    "    # calculate the mean\n",
    "    mean_macro_time = np.mean(macro_times)\n",
    "\n",
    "    ##############################\n",
    "    ### Number of Photons calc ###\n",
    "    ##############################\n",
    "    num_photons = len(burst)\n",
    "\n",
    "    #######################\n",
    "    ### Count Rate calc ###\n",
    "    #######################\n",
    "    count_rate = num_photons / duration\n",
    "\n",
    "    #################################\n",
    "    ### Duration (green)(ms) calc ###\n",
    "    #################################\n",
    "    \n",
    "    # Assuming you have your list of indexes\n",
    "    list_of_indexes = burst\n",
    "    \n",
    "    # Create boolean masks for channels 0 and 2 to select the green channels\n",
    "    mask_channel_0 = routing_channels[list_of_indexes] == 0\n",
    "    mask_channel_2 = routing_channels[list_of_indexes] == 2\n",
    "    \n",
    "    # Use boolean masks to filter the indexes\n",
    "    indexes_channel_0 = np.array(list_of_indexes)[mask_channel_0]\n",
    "    indexes_channel_2 = np.array(list_of_indexes)[mask_channel_2]\n",
    "    \n",
    "    # Output the filtered indexes\n",
    "    # print(\"Indexes corresponding to channel 0:\", indexes_channel_0)\n",
    "    # print(\"Indexes corresponding to channel 2:\", indexes_channel_2)\n",
    "    \n",
    "    # Find the minimum and maximum indexes across both resulting arrays. These are first and last green \n",
    "    # photon in this burst. handle situations where only one channel has photons or none have photons.\n",
    "    if len(indexes_channel_0) > 0 and len(indexes_channel_2) > 0:\n",
    "        first_green_photon = min(np.min(indexes_channel_0), np.min(indexes_channel_2))\n",
    "        last_green_photon = max(np.max(indexes_channel_0), np.max(indexes_channel_2))\n",
    "    elif len(indexes_channel_0) >= 2:\n",
    "        first_green_photon = np.min(indexes_channel_0)\n",
    "        last_green_photon = np.max(indexes_channel_0)\n",
    "    elif len(indexes_channel_2) >= 2:\n",
    "        first_green_photon = np.min(indexes_channel_2)\n",
    "        last_green_photon = np.max(indexes_channel_2)\n",
    "    else:\n",
    "        first_green_photon = None\n",
    "        last_green_photon = None\n",
    "\n",
    "    # Calculate duration or set as NaN\n",
    "    if first_green_photon != None and last_green_photon != None:\n",
    "        lgp_time = all_macro_times[last_green_photon]*macro_res + all_micro_times[last_green_photon]*micro_res\n",
    "        fgp_time = all_macro_times[first_green_photon]*macro_res + all_micro_times[first_green_photon]*micro_res\n",
    "        lgp_time_ms = lgp_time*1000\n",
    "        fgp_time_ms = fgp_time*1000\n",
    "        duration_green = (lgp_time_ms - fgp_time_ms)\n",
    "    else:\n",
    "        duration_green = np.nan\n",
    "\n",
    "    #########################################\n",
    "    #### Mean Macro Time (green)(ms) calc ###\n",
    "    #########################################\n",
    "    \n",
    "    # get all the macro times corresponding to the photons in this burst\n",
    "    macro_times_ch0 = all_macro_times[indexes_channel_0]*macro_res*1000\n",
    "    macro_times_ch2 = all_macro_times[indexes_channel_2]*macro_res*1000\n",
    "    \n",
    "    # Concatenate the arrays along the appropriate axis\n",
    "    combined_macro_times = np.concatenate([macro_times_ch0, macro_times_ch2], axis=0)\n",
    "    \n",
    "    # calculate the mean\n",
    "    mean_macro_time_green = np.mean(combined_macro_times, axis=0)\n",
    "\n",
    "    #################################\n",
    "    ### Number of Photons (green) ###\n",
    "    #################################\n",
    "    num_photons_gr = len(indexes_channel_0) + len(indexes_channel_2)\n",
    "\n",
    "    ########################\n",
    "    ### Green Count Rate ###\n",
    "    ########################\n",
    "    count_rate_gr = num_photons_gr / duration_green\n",
    "\n",
    "    bur_new_row = {'First Photon': [first_photon],'Last Photon': [last_photon],\n",
    "               'Duration (ms)': [duration],'Mean Macro Time (ms)': [mean_macro_time],\n",
    "               'Number of Photons': [num_photons], 'Count Rate (kHz)': [count_rate],\n",
    "               'Duration (green) (ms)': [duration_green], \n",
    "               'Mean Macro Time (green) (ms)': [mean_macro_time_green],\n",
    "               'Number of Photons (green)': [num_photons_gr],\n",
    "               'Green Count Rate (kHz)': count_rate_gr\n",
    "              }\n",
    "    \n",
    "    bur_new_record = pd.DataFrame.from_dict(bur_new_row)\n",
    "    \n",
    "    ### append record to df\n",
    "    bi4_bur_df = pd.concat([bi4_bur_df, bur_new_record], ignore_index=True)\n",
    "\n",
    "    ######################## .bg4 calculations\n",
    "\n",
    "    ################\n",
    "    ### Ng-p-all ###\n",
    "    ################\n",
    "\n",
    "    # Find the events in the burst where the corresponding channel is 2 and 0 < micro time < 12499 (prompt) using list comprehension \n",
    "    bg4_channel_2_photons = [index for index in burst if routing_channels[index] == 2 and \n",
    "                                 bg4_micro_time_min < all_micro_times[index] < bg4_micro_time_max]\n",
    "    bg4_channel_2_count = len(bg4_channel_2_photons)\n",
    "\n",
    "    # append bg4 chn 2 photons to total list for MLE estimations later\n",
    "    bg4_channel_2_photons_total.extend(bg4_channel_2_photons)\n",
    "\n",
    "\n",
    "    ################\n",
    "    ### Ng-s-all ###\n",
    "    ################\n",
    "\n",
    "    # Find the indices in burst where the corresponding channel is 0 using list comprehension\n",
    "    bg4_channel_0_photons = [index for index in burst if routing_channels[index] == 0 and \n",
    "                                 bg4_micro_time_min < all_micro_times[index] < bg4_micro_time_max]\n",
    "    bg4_channel_0_count = len(bg4_channel_0_photons)\n",
    "\n",
    "    # append bg4 chn 0 photons to total list for MLE estimations later\n",
    "    bg4_channel_0_photons_total.extend(bg4_channel_0_photons)\n",
    "\n",
    "    ##############################################\n",
    "    ### Number of Photons (fit window) (green) ###\n",
    "    ##############################################\n",
    "\n",
    "    bg4_total_count = bg4_channel_2_count + bg4_channel_0_count\n",
    "\n",
    "    ##############################\n",
    "    ### r Experimental (green) ###\n",
    "    ##############################\n",
    "\n",
    "    bg4_rexp = calc_flour_aniso(g_factor, bg4_channel_2_count, bg4_channel_0_count, l1_japan_corr, l2_japan_corr)\n",
    "\n",
    "    #########################\n",
    "    ### r Scatter (green) ###\n",
    "    #########################\n",
    "\n",
    "    bg4_rscat = calc_flour_aniso(g_factor, bg4_channel_2_count - bg4_bkg_para, bg4_channel_0_count - bg4_bkg_perp, l1_japan_corr, l2_japan_corr)\n",
    "\n",
    "    ### create and store new record for this burst \n",
    "\n",
    "    bg4_new_row = {'Ng-p-all': [bg4_channel_2_count],\n",
    "                   'Ng-s-all': [bg4_channel_0_count],\n",
    "                   'Number of Photons (fit window) (green)' : [bg4_total_count],\n",
    "                   'r Scatter (green)' : [bg4_rscat],\n",
    "                   'r Experimental (green)' : [bg4_rexp]\n",
    "                  }\n",
    "    bg4_new_record = pd.DataFrame.from_dict(bg4_new_row)\n",
    "    \n",
    "    ### append record to df\n",
    "    bg4_df = pd.concat([bg4_df, bg4_new_record], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d4b90a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17530, 10136,  1345, ...,  1057,  5935,  2449], dtype=uint16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_micro_times_irf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "559e9737",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'counts_bkg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 67\u001b[0m\n\u001b[1;32m     62\u001b[0m corrections \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([period, g, l1, l2, conv_stop])\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# create MParam structure that contains all parameters for fitting\u001b[39;00m\n\u001b[1;32m     65\u001b[0m m_param \u001b[38;5;241m=\u001b[39m tttrlib\u001b[38;5;241m.\u001b[39mCreateMParam(\n\u001b[1;32m     66\u001b[0m     irf\u001b[38;5;241m=\u001b[39mcounts_irf,\n\u001b[0;32m---> 67\u001b[0m     background\u001b[38;5;241m=\u001b[39m\u001b[43mcounts_bkg\u001b[49m,\n\u001b[1;32m     68\u001b[0m     data\u001b[38;5;241m=\u001b[39mcounts\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint32),\n\u001b[1;32m     69\u001b[0m     corrections\u001b[38;5;241m=\u001b[39mcorrections,\n\u001b[1;32m     70\u001b[0m     dt\u001b[38;5;241m=\u001b[39mdt\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m tau, gamma, r0, rho \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4.\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.38\u001b[39m, \u001b[38;5;241m1.5\u001b[39m\n\u001b[1;32m     74\u001b[0m bifl_scatter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'counts_bkg' is not defined"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "### Count Calculations ###\n",
    "##########################\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons_total]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "micro_times_channel_0_shifted = micro_times_channel_0 + 12499 # 12499 is shift between the 2 arrays, so we can concatenate them \n",
    "\n",
    "chn0_shifted_plus_chn2 = np.concatenate([micro_times_channel_0_shifted, micro_times_channel_2])\n",
    "\n",
    "# Get the counts and bins\n",
    "counts, bins = np.histogram(chn0_shifted_plus_chn2, bins=num_bins, range=(min(chn0_shifted_plus_chn2), max(chn0_shifted_plus_chn2)))\n",
    "\n",
    "\n",
    "########################\n",
    "### IRF Calculations ###\n",
    "########################\n",
    "\n",
    "# Find the indices in IRF where the corresponding channel is 2 and 0 \n",
    "channel_2_photons_irf = [index for index, channel in enumerate(routing_channels_irf) if channel == 2 and \n",
    "                                bg4_micro_time_min < all_micro_times_irf[index] < bg4_micro_time_max]\n",
    "channel_0_photons_irf = [index for index, channel in enumerate(routing_channels_irf) if channel == 0 and \n",
    "                                bg4_micro_time_min < all_micro_times_irf[index] < bg4_micro_time_max]\n",
    "\n",
    "# Add 12499 to parallel photon microtimes\n",
    "micro_times_channel_0_irf = all_micro_times_irf[channel_0_photons_irf]\n",
    "micro_times_channel_0_shifted_irf = micro_times_channel_0_irf + 12499\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2_irf = all_micro_times_irf[channel_2_photons_irf]\n",
    "\n",
    "# Combine channel 0 and 2 irf data for plotting\n",
    "chn0_shifted_plus_chn2_irf = np.concatenate([micro_times_channel_0_shifted_irf, micro_times_channel_2_irf])\n",
    "\n",
    "# Plot the irf data\n",
    "counts_irf, bins_irf = np.histogram(chn0_shifted_plus_chn2_irf, bins=num_bins, range=(min(chn0_shifted_plus_chn2_irf), max(chn0_shifted_plus_chn2_irf)))\n",
    "\n",
    "\n",
    "####################\n",
    "### Fit23 Counts ###\n",
    "####################\n",
    "\n",
    "dt = 25000/num_bins/1000\n",
    "\n",
    "# setup some parameters\n",
    "n_channels = 128\n",
    "n_corrections = 5\n",
    "n_photons = 120\n",
    "irf_position_p = 2.0\n",
    "irf_position_s = 18.0\n",
    "irf_width = 0.25\n",
    "period, g, l1, l2, conv_stop = 32, 1.0, 0.1, 0.1, n_channels // 2 - 1\n",
    "tau, gamma, r0, rho = 2.0, 0.01, 0.38, 1.2\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "\n",
    "conv_stop = min(num_bins, conv_stop)\n",
    "param = np.array([tau, gamma, r0, rho])\n",
    "corrections = np.array([period, g, l1, l2, conv_stop])\n",
    "\n",
    "# create MParam structure that contains all parameters for fitting\n",
    "m_param = tttrlib.CreateMParam(\n",
    "    irf=counts_irf,\n",
    "    background=counts_bkg,\n",
    "    data=counts.astype(np.int32),\n",
    "    corrections=corrections,\n",
    "    dt=dt\n",
    ")\n",
    "\n",
    "tau, gamma, r0, rho = 4., 0.01, 0.38, 1.5\n",
    "bifl_scatter = -1\n",
    "p_2s = 0\n",
    "x = np.zeros(8, dtype=np.float64)\n",
    "x[:6] = [tau, gamma, r0, rho, bifl_scatter, p_2s]\n",
    "\n",
    "# test fitting\n",
    "fixed = np.array([0, 1, 1, 1], dtype=np.int16)\n",
    "chi2 = tttrlib.DecayFit23.fit(x, fixed, m_param)\n",
    "\n",
    "m = np.array([m for m in m_param.get_model()])\n",
    "pylab.plot(m, label = 'm')\n",
    "pylab.plot(counts, label = 'counts')\n",
    "pylab.plot(counts_irf / max(counts_irf) * max(counts), label = 'irf')\n",
    "plt.legend()\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac8c70-9e08-4fb1-8db1-573ffb1577fa",
   "metadata": {},
   "source": [
    "## Save the bi4_bur, bg4, br4 and by4 to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d93c720e-9138-4dc5-917e-d442828970c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_p' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# put output directory at the start\u001b[39;00m\n\u001b[1;32m      3\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtests\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mburstid_selection_viz_tool\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 4\u001b[0m bur_filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(\u001b[43mfile_p\u001b[49m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m bur_filepath \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_directory,bur_filename) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.bur\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m bi4_bur_df\u001b[38;5;241m.\u001b[39mto_csv(bur_filepath, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, float_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%.6f\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Save without index\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'file_p' is not defined"
     ]
    }
   ],
   "source": [
    "# put output directory at the start\n",
    "\n",
    "output_directory = r'..\\tests\\burstid_selection_viz_tool'\n",
    "bur_filename = os.path.splitext(os.path.basename(file_p))[0]\n",
    "bur_filepath = os.path.join(output_directory,bur_filename) + \".bur\"\n",
    "bi4_bur_df.to_csv(bur_filepath, sep='\\t', index=False, float_format='%.6f')  # Save without index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ee6ca65b-9a59-43d3-984e-718ffb74027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bur_filename = os.path.splitext(os.path.basename(file_p))[0]\n",
    "bur_filepath = os.path.join(output_directory,bur_filename) + \".bg4\"\n",
    "bg4_df.to_csv(bur_filepath, sep='\\t', index=False, float_format='%.6f')  # Save without index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8b587-293b-4c10-b0c2-c6e852a12d05",
   "metadata": {},
   "source": [
    "## Mean Micro Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a52e01b8-ac80-4d24-b0d4-eec4b7bc6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean micro time for channels 0 and 2 photons: 2772.3396007151373\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the arrays: bg4_channel_0_photons, bg4_channel_2_photons, and all_micro_times\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons_total]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "# Combine micro times for both channels\n",
    "combined_micro_times = np.concatenate([micro_times_channel_0, micro_times_channel_2])\n",
    "\n",
    "# Calculate the mean micro time for combined channels\n",
    "mean_micro_time_combined = np.mean(combined_micro_times)\n",
    "\n",
    "# Output the combined mean micro time\n",
    "print(\"Combined mean micro time for channels 0 and 2 photons:\", mean_micro_time_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f37ac5d6-598d-449a-bd42-d83bae106fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean micro time for channels 0 and 2 photons: 2780.4363409786574\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the arrays: bg4_channel_0_photons, bg4_channel_2_photons, and all_micro_times\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons_total]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "# Calculate the mean micro time for channel 0 photons\n",
    "mean_micro_time_channel_0 = np.mean(micro_times_channel_0)\n",
    "\n",
    "# Calculate the mean micro time for channel 2 photons\n",
    "mean_micro_time_channel_2 = np.mean(micro_times_channel_2)\n",
    "\n",
    "# Calculate the mean of the means\n",
    "combined_mean_micro_time = np.mean([mean_micro_time_channel_0, mean_micro_time_channel_2])\n",
    "\n",
    "# Output the combined mean micro time\n",
    "print(\"Combined mean micro time for channels 0 and 2 photons:\", combined_mean_micro_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7f1b8-72ab-4e59-bcb1-76d89f742897",
   "metadata": {},
   "source": [
    "## Visualize the micro time decay curves for the entire bg4 data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe151c2c-4a3e-416c-a5b6-77be96dfc1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This data can be used prior to performing burst by burst mle fitting to determine the initial fitting parameters tau, gamma, r0, and rho.\n",
    "\n",
    "### plot the histogram of the ptu file\n",
    "\n",
    "# Add 12499 to parallel photon microtimes\n",
    "micro_times_channel_0_shifted = micro_times_channel_0 + 12499\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "chn0_shifted_plus_chn2 = np.concatenate([micro_times_channel_0_shifted, micro_times_channel_2])\n",
    "\n",
    "num_bins = 128\n",
    "# Plot the histogram with 128 bins\n",
    "counts, bins, _ = plt.hist(chn0_shifted_plus_chn2, bins=num_bins)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Extract the binned data\n",
    "# print(\"Counts for each bin:\", counts)\n",
    "# print(\"Bin edges:\", bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a2bd09a3-687e-4544-8d3f-6c11b4a8427b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the histogram of the irf\n",
    "\n",
    "# Find the indices in IRF where the corresponding channel is 2 and 0 \n",
    "channel_2_photons_irf = [index for index, channel in enumerate(routing_channels_irf) if channel == 2 and \n",
    "                                 bg4_micro_time_min < all_micro_times_irf[index] < bg4_micro_time_max]\n",
    "channel_0_photons_irf = [index for index, channel in enumerate(routing_channels_irf) if channel == 0 and \n",
    "                                 bg4_micro_time_min < all_micro_times_irf[index] < bg4_micro_time_max]\n",
    "\n",
    "# Add 12499 to parallel photon microtimes\n",
    "micro_times_channel_0_irf = all_micro_times_irf[channel_0_photons_irf]\n",
    "micro_times_channel_0_shifted_irf = micro_times_channel_0_irf + 12499\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2_irf = all_micro_times_irf[channel_2_photons_irf]\n",
    "\n",
    "# Combine channel 0 and 2 irf data for plotting\n",
    "chn0_shifted_plus_chn2_irf = np.concatenate([micro_times_channel_0_shifted_irf, micro_times_channel_2_irf])\n",
    "\n",
    "# Plot the irf data\n",
    "counts_irf, bins_irf, _ = plt.hist(chn0_shifted_plus_chn2_irf, bins=num_bins)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Extract the binned data\n",
    "# print(\"Counts for each bin:\", counts)\n",
    "# print(\"Bin edges:\", bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d966a20c-57d3-4414-b5e9-4795b823849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the histogram of the bkg\n",
    "\n",
    "# Find the indices in bkg where the corresponding channel is 2 and 0 \n",
    "channel_2_photons_bkg = [index for index, channel in enumerate(routing_channels_bkg) if channel == 2 and \n",
    "                                 bg4_micro_time_min < all_micro_times_bkg[index] < bg4_micro_time_max]\n",
    "channel_0_photons_bkg = [index for index, channel in enumerate(routing_channels_bkg) if channel == 0 and \n",
    "                                 bg4_micro_time_min < all_micro_times_bkg[index] < bg4_micro_time_max]\n",
    "\n",
    "# Add 12499 to parallel photon microtimes\n",
    "micro_times_channel_0_bkg = all_micro_times_bkg[channel_0_photons_bkg]\n",
    "micro_times_channel_0_shifted_bkg = micro_times_channel_0_bkg + 12499\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2_bkg = all_micro_times_bkg[channel_2_photons_bkg]\n",
    "\n",
    "# Combine channel 0 and 2 bkg data for plotting\n",
    "chn0_shifted_plus_chn2_bkg = np.concatenate([micro_times_channel_0_shifted_bkg, micro_times_channel_2_bkg])\n",
    "\n",
    "# Plot the bkg data\n",
    "counts_bkg, bins_bkg, _ = plt.hist(chn0_shifted_plus_chn2_bkg, bins=num_bins)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Extract the binned data\n",
    "# print(\"Counts for each bin:\", counts)\n",
    "# print(\"Bin edges:\", bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c532482-acde-405c-8ff4-f27b0853f4fc",
   "metadata": {},
   "source": [
    "## Estimate tau, gamma, r0, and rho using fit23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9e9984bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "\n",
    "dt = 25000/num_bins/1000\n",
    "\n",
    "# setup some parameters\n",
    "n_channels = 128\n",
    "n_corrections = 5\n",
    "n_photons = 120\n",
    "irf_position_p = 2.0\n",
    "irf_position_s = 18.0\n",
    "irf_width = 0.25\n",
    "period, g, l1, l2, conv_stop = 32, 1.0, 0.1, 0.1, n_channels // 2 - 1\n",
    "tau, gamma, r0, rho = 2.0, 0.01, 0.38, 1.2\n",
    "np.random.seed(0)\n",
    "\n",
    "conv_stop = min(num_bins, conv_stop)\n",
    "param = np.array([tau, gamma, r0, rho])\n",
    "corrections = np.array([period, g, l1, l2, conv_stop])\n",
    "\n",
    "# create MParam structure that contains all parameters for fitting\n",
    "m_param = tttrlib.CreateMParam(\n",
    "    irf=counts_irf,\n",
    "    background=counts_bkg,\n",
    "    data=counts.astype(np.int32),\n",
    "    corrections=corrections,\n",
    "    dt=dt\n",
    ")\n",
    "\n",
    "tau, gamma, r0, rho = 4., 0.01, 0.38, 1.5\n",
    "bifl_scatter = -1\n",
    "p_2s = 0\n",
    "x = np.zeros(8, dtype=np.float64)\n",
    "x[:6] = [tau, gamma, r0, rho, bifl_scatter, p_2s]\n",
    "\n",
    "# test fitting\n",
    "fixed = np.array([0, 1, 1, 1], dtype=np.int16)\n",
    "chi2 = tttrlib.DecayFit23.fit(x, fixed, m_param)\n",
    "\n",
    "m = np.array([m for m in m_param.get_model()])\n",
    "pylab.plot(m, label = 'm')\n",
    "pylab.plot(counts, label = 'counts')\n",
    "pylab.plot(counts_irf / max(counts_irf) * max(counts), label = 'irf')\n",
    "plt.legend()\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f8166ff1-6175-4b97-bea6-0bab217f577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([ 1.66844577e+000,  8.56586262e-312,  3.80000000e-001,\n",
      "        2.42986235e+000, -1.00000000e+000,  1.00000000e+000,\n",
      "        2.25299725e-001,  4.10180612e-001]), 'fixed': array([0, 0, 1, 0], dtype=int16), 'twoIstar': 280.4975330245197, 'model': array([1.27435581e+02, 1.90855188e+03, 8.89099105e+03, 1.47650957e+04,\n",
      "       1.49955176e+04, 1.32348830e+04, 1.14397214e+04, 9.90255428e+03,\n",
      "       8.58418738e+03, 7.45167233e+03, 6.47727321e+03, 5.63760904e+03,\n",
      "       4.91294131e+03, 4.28658181e+03, 3.74439979e+03, 3.27441153e+03,\n",
      "       2.86643817e+03, 2.51182013e+03, 2.20317854e+03, 1.93421569e+03,\n",
      "       1.69954795e+03, 1.49456562e+03, 1.31531538e+03, 1.15840137e+03,\n",
      "       1.02090202e+03, 9.00299840e+02, 7.94422248e+02, 7.01391481e+02,\n",
      "       6.19582211e+02, 5.47585592e+02, 4.84178731e+02, 4.28298714e+02,\n",
      "       3.79020472e+02, 3.35537896e+02, 2.97147688e+02, 2.63235543e+02,\n",
      "       2.33264289e+02, 2.06763706e+02, 1.83321769e+02, 1.62577094e+02,\n",
      "       1.44212432e+02, 1.27949033e+02, 1.13541778e+02, 1.00774954e+02,\n",
      "       8.94585875e+01, 7.94252616e+01, 7.05273402e+01, 6.26345537e+01,\n",
      "       5.56318907e+01, 4.94177574e+01, 4.39023672e+01, 3.90063321e+01,\n",
      "       3.46594278e+01, 3.07995110e+01, 2.73715686e+01, 2.43268826e+01,\n",
      "       2.16222958e+01, 1.92195650e+01, 1.70847924e+01, 1.51879236e+01,\n",
      "       1.35023057e+01, 1.20042965e+01, 1.06729204e+01, 9.48956380e+00,\n",
      "       1.18748230e+02, 5.67081384e+02, 2.22399839e+03, 3.68489532e+03,\n",
      "       3.95592943e+03, 3.79540410e+03, 3.47278310e+03, 3.16848353e+03,\n",
      "       2.88358444e+03, 2.61848972e+03, 2.37310224e+03, 2.14696105e+03,\n",
      "       1.93934860e+03, 1.74937426e+03, 1.57603860e+03, 1.41828255e+03,\n",
      "       1.27502448e+03, 1.14518765e+03, 1.02772024e+03, 9.21609432e+02,\n",
      "       8.25890930e+02, 7.39654955e+02, 6.62049542e+02, 5.92281802e+02,\n",
      "       5.29617672e+02, 4.73380574e+02, 4.22949291e+02, 3.77755325e+02,\n",
      "       3.37279925e+02, 3.01050927e+02, 2.68639531e+02, 2.39657087e+02,\n",
      "       2.13751949e+02, 1.90606455e+02, 1.69934041e+02, 1.51476522e+02,\n",
      "       1.35001543e+02, 1.20300210e+02, 1.07184894e+02, 9.54872019e+01,\n",
      "       8.50561250e+01, 7.57563323e+01, 6.74666205e+01, 6.00785000e+01,\n",
      "       5.34949118e+01, 4.76290637e+01, 4.24033781e+01, 3.77485408e+01,\n",
      "       3.36026429e+01, 2.99104083e+01, 2.66224973e+01, 2.36948818e+01,\n",
      "       2.10882834e+01, 1.87676687e+01, 1.67017977e+01, 1.48628183e+01,\n",
      "       1.32259031e+01, 1.17689251e+01, 1.04721668e+01, 9.31805984e+00,\n",
      "       8.29095347e+00, 7.37690638e+00, 6.56350153e+00, 5.83968060e+00])}\n"
     ]
    }
   ],
   "source": [
    "# estimate tau, gamma, r0, and rho using fit23\n",
    "\n",
    "# is the width of the bins in time\n",
    "dt = 25000/num_bins/1000\n",
    "\n",
    "# period is pulse duration of the laser (50ns)\n",
    "period = 1/macro_res/np.power(10,6)\n",
    "\n",
    "# irf_np = np.zeros_like(counts)\n",
    "\n",
    "# irf_np = np.array([260, 158200000, 155, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 22, 107400000, 830, 10, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0], dtype=np.float64)\n",
    "\n",
    "# count_bkg = np.zeros_like(counts_irf)\n",
    "\n",
    "#### SET IRF to zero from channel 5 to channl 64 and 70 to 128 to remove IRF background\n",
    "counts_irf[5:64] = 0\n",
    "counts_irf[70:128] = 0\n",
    "\n",
    "fit23 = tttrlib.Fit23(\n",
    "    dt=dt,\n",
    "    # irf=irf_np,\n",
    "    irf=counts_irf,\n",
    "    background=counts_bkg,\n",
    "    period=period,\n",
    "    g_factor=g_factor,\n",
    "    l1=l1_japan_corr, l2=l2_japan_corr,\n",
    "    convolution_stop = 5,\n",
    "    p2s_twoIstar_flag = True\n",
    ")\n",
    "\n",
    "# normalize counts\n",
    "norm_counts = counts/np.max(counts)\n",
    "\n",
    "tau, gamma, r0, rho = 4.0, 0.1, 0.38, 0.2\n",
    "x0 = np.array([tau, gamma, r0, rho])\n",
    "fixed = np.array([0, 0, 1, 0])\n",
    "r2 = fit23(data=counts, initial_values=x0, fixed=fixed, include_model = True)\n",
    "print(r2)\n",
    "# output key from DecayFit23.h on tttrlib github.\n",
    "# x[0] fluorescence lifetime - tau;\n",
    "# x[1] fraction of scattered light - gamma;\n",
    "# x[2] fundamental anisotropy - r0\n",
    "# x[3] rotational time - rho;\n",
    "# x[4] softbifl - flag specifying the type of bifl fit (not used here)\n",
    "# x[5] p2s_twoIstar - flag specifying the type of chi2 calculation (not used here)\n",
    "# x[6] background corrected anisotropy\n",
    "# x[7] anisotropy without background correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0c4f35a6-2e36-4246-9575-bfcfa1f8f561",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tttrlib.tttrlib.MParam; proxy of <Swig Object of type 'MParam *' at 0x0000024B91363D20> >\n"
     ]
    }
   ],
   "source": [
    "import pylab as p\n",
    "\n",
    "# estimate using MLE example 2\n",
    "\n",
    "# is the width of the bins in time\n",
    "dt = 25000/num_bins/1000\n",
    "\n",
    "# period is pulse duration of the laser (50ns)\n",
    "period = 1/macro_res/np.power(10,6)\n",
    "\n",
    "# irf_np = np.zeros_like(counts)\n",
    "\n",
    "# irf_np = np.array([260, 158200000, 155, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 22, 107400000, 830, 10, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "#                    0, 0, 0, 0], dtype=np.float64)\n",
    "\n",
    "count_bkg = np.zeros_like(counts_irf)\n",
    "\n",
    "# conv_stop = num_bins // 2 /- 1\n",
    "conv_stop = 12\n",
    "\n",
    "\n",
    "corrections = np.array([period, g_factor, l1_japan_corr, l2_japan_corr, conv_stop])\n",
    "m_param = tttrlib.CreateMParam(\n",
    "    irf=counts_irf,\n",
    "    background=counts_bkg,\n",
    "    data = counts.astype(np.int32),\n",
    "    corrections = corrections,\n",
    "    dt=dt\n",
    ")\n",
    "\n",
    "\n",
    "tau, gamma, r0, rho = 4.0, 0.01, 0.38, 1.5\n",
    "bifl_scatter = -1\n",
    "p_2s = 0\n",
    "\n",
    "x = np.zeros(8, dtype=np.float64)\n",
    "x[:6] = [tau, gamma, r0, rho, bifl_scatter, p_2s]\n",
    "\n",
    "fixed = np.array([0, 1, 1, 1], dtype=np.int16)\n",
    "chi2 = tttrlib.DecayFit23.fit(x, fixed, m_param)\n",
    "\n",
    "m = np.array([m for m in m_param.get_model()])\n",
    "p.plot(m)\n",
    "p.plot(counts)\n",
    "p.plot(counts_irf / max(counts_irf) * max(counts))\n",
    "p.show()\n",
    "\n",
    "print(m_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e797ede-5fd0-4656-b014-03db6d53ab94",
   "metadata": {},
   "source": [
    "## Use MLE data to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ddb2554-e3b8-43bb-b45d-ebe5e431ba30",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_stop' is not defined",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[42], line 9\u001b[1;36m\n\u001b[1;33m    corrections = np.array([period, g_factor, l1_japan_corr, l2_japan_corr, conv_stop])\u001b[1;36m\n",
      "\u001b[1;31mNameError\u001b[0m\u001b[1;31m:\u001b[0m name 'conv_stop' is not defined\n"
     ]
    }
   ],
   "source": [
    "# from plot fit23 benchmark article online\n",
    "\n",
    "# conv_stop = num_bins // 2 - 1 defined above\n",
    "#param = np.array([r2['x'][0], 0.08, r2['x'][2], r2['x'][3]])\n",
    "# param = np.array([3.03, 0.08, 0.38, 0.2, -1, 0])\n",
    "param = np.array([400, x[1], x[2], x[3]])\n",
    "\n",
    "# param = np.array([tau, gamma, r0, rho])\n",
    "corrections = np.array([period, g_factor, l1_japan_corr, l2_japan_corr, conv_stop])\n",
    "# model = np.zeros_like(irf_np)\n",
    "model = np.zeros_like(counts_irf)\n",
    "# bg = np.zeros_like(irf_np)\n",
    "# tttrlib.DecayFit23.modelf(param, irf_np, bg, dt, corrections, model)\n",
    "tttrlib.DecayFit23.modelf(param, counts_irf, counts_bkg, dt, corrections, model)\n",
    "n_photons = len(chn0_shifted_plus_chn2) # normalize for some reason. see tttrlib fit23 benchmark article.\n",
    "model *= n_photons / np.sum(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7cc898f6-cbfd-4f39-9461-95e62b475fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x19394f3d190>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.semilogy([x for x in fit23.data], label='Data')\n",
    "plt.semilogy([x for x in fit23.irf], label='IRF')\n",
    "plt.semilogy([x for x in fit23.model], label='Model')\n",
    "# Set the y-axis limits\n",
    "plt.ylim(10, 100000)\n",
    "# plt.title.set_text(r'Example decay')\n",
    "plt.ylabel(r'log(Counts)')\n",
    "plt.xlabel(r'Channel Nbr.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "aefe5c1a-fd32-4a9b-896e-8abd47e59724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MLE to fit mean micro time decays. extract model parameters. use to fit each burst and extract lifetimes from bursts.\n",
    "# how many bursts per second are selected and how many bursts per seconds after filter. before and after thresdhol, what is the burst per second? Single molecule order of mag is 2-3 burst per second byt the time you have lifetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05370953-eba3-4dda-9e12-090c527ea2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "physics-64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
