{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf0902c-cc14-4ae0-9dc3-0fa7c2491a8d",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3052ae27-0a40-4d08-b145-f84eb89e1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import tttrlib\n",
    "\n",
    "import os\n",
    "\n",
    "from feda_tools import twodim_hist as tdh\n",
    "from feda_tools import utilities as utils\n",
    "from feda_tools import analysis as an\n",
    "\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import halfnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e9ad2-aec6-4ec5-908c-00ddb355f94c",
   "metadata": {},
   "source": [
    "## Declare required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dac192f1-4cf3-4b97-b9ac-9551edd81bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flour_aniso(g_factor, intensity_para, intensity_perp, l1_japan_corr, l2_japan_corr):\n",
    "    ### Fluorescence Anisotropy calculation. see equation 7 in Kudryavtsev, V., Sikor, M., Kalinin, S., Mokranjac, D., Seidel, C.A.M. and Lamb, D.C. (2012), \n",
    "    ### Combining MFD and PIE for Accurate Single-Pair FÃ¶rster Resonance Energy Transfer Measurements. ChemPhysChem, 13: 1060-1078. https://doi.org/10.1002/cphc.201100822\n",
    "    \n",
    "    return (g_factor * intensity_para - intensity_perp) / ((1 - 3 * l2_japan_corr) * g_factor * intensity_para + (2 - 3 * l1_japan_corr) * intensity_perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15b425-cdea-4f49-9141-974fccc68131",
   "metadata": {},
   "source": [
    "## Load target PTU file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "56416358-d044-42cf-acff-e49b575d19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PTU Files\n",
    "# file_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30')\n",
    "# bid_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30/BIDs_30ph')\n",
    "\n",
    "### for testing purposes ###\n",
    "# file_path = pathlib.Path('C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu')\n",
    "\n",
    "#total time 816.9 seconds for this file\n",
    "file_p ='C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c37af5-7223-4a4f-b10e-0eacfb495a03",
   "metadata": {},
   "source": [
    "## Initialize tttrlib data and extract important global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0bf7205d-e2f0-42bd-92a1-916a43bf2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tttrlib.TTTR(file_p, 'PTU')\n",
    "all_macro_times = data.macro_times\n",
    "all_micro_times = data.micro_times\n",
    "routing_channels =  data.routing_channels\n",
    "\n",
    "#in seconds. usually the first plots are in ms to see the bursts.\n",
    "macro_res =data.get_header().macro_time_resolution\n",
    "micro_res = data.get_header().micro_time_resolution\n",
    "\n",
    "# total duration in seconds\n",
    "total_duration = all_macro_times[-1] * macro_res\n",
    "\n",
    "# define analysis window for subset of PTU\n",
    "min_event = 0\n",
    "max_event = 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe224-8caf-46a3-9a0d-532345adaab1",
   "metadata": {},
   "source": [
    "## Determine analysis settings for bur, bg4, by4, and br4 calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "681c0cfe-13f2-4e14-9e37-a3183b60673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# photon count threshold for burst selection\n",
    "min_photon_count = 60\n",
    "\n",
    "# this window changes for br4 and by4\n",
    "# bg4 parameters\n",
    "bg4_micro_time_min = 0\n",
    "bg4_micro_time_max = 12499\n",
    "\n",
    "# flourescence anisotropy parameters\n",
    "g_factor = 2.62278093\n",
    "l1_japan_corr = 0.0308\n",
    "l2_japan_corr = 0.0368\n",
    "\n",
    "# bkg signals required for r Scatter calculations\n",
    "bg4_bkg_para = 0\n",
    "bg4_bkg_perp = 0\n",
    "\n",
    "\n",
    "# MLE parameters\n",
    "num_bins = 128\n",
    "bin_width = macro_res/micro_res/num_bins/1000 # in nanoseconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b2f2-15a8-4f05-9bd5-000d6edeaf5b",
   "metadata": {},
   "source": [
    "## Calculate interphoton arrival time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "48e6c51c-a284-49fc-a41f-04b03dc3f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through macro and micro times to calculate delta time between photon events\n",
    "arr_size = len(all_macro_times) - 1\n",
    "photon_time_intervals = np.zeros(arr_size, dtype = np.float64)\n",
    "lw = 0.25\n",
    "for i in range(0, len(photon_time_intervals)):\n",
    "    photon_1 = (all_macro_times[i]*macro_res) + (all_micro_times[i]*micro_res)\n",
    "    photon_2 = (all_macro_times[i+1]*macro_res) + (all_micro_times[i+1]*micro_res)\n",
    "    photon_time_intervals[i] = (photon_2 - photon_1)*1000\n",
    "\n",
    "# create photon ID array\n",
    "photon_ids = np.arange(1, arr_size + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "01fb470b-fc0e-49f6-98e5-6d16f39467f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3073616332567326\n"
     ]
    }
   ],
   "source": [
    "print(photon_time_intervals[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26a8f8-c3e7-4bae-8ffb-d13b35b816b9",
   "metadata": {},
   "source": [
    "## Plot the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d38e86b8-8598-443e-83b6-561d1fd0b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot interactive\n",
    "%matplotlib qt\n",
    "\n",
    "# raw data, no running average\n",
    "plt.plot(photon_ids, np.log10(photon_time_intervals), linewidth = 0.25)\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0016ec6-6c36-498d-a6b4-a2e2f1410c95",
   "metadata": {},
   "source": [
    "## Plot the log of the running average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a02b49a-8a02-49a7-b0f7-84055deb7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(data, window_size):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, mode='valid')\n",
    "\n",
    "# Create a NumPy array\n",
    "data = photon_time_intervals\n",
    "\n",
    "# Set the window size for the running average\n",
    "window_size = 30\n",
    "\n",
    "# Calculate the running average\n",
    "running_avg = running_average(data, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f4625dea-483e-47b1-9d90-ea31924d5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the running average\n",
    "# plt.plot(data, label='Original Data')\n",
    "lw = 0.25\n",
    "xarr = np.arange(window_size - 1, len(data))\n",
    "logrunavg = np.log10(running_avg)\n",
    "plt.plot(xarr, logrunavg, label='Running Average', linewidth = lw)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a3ff9-882b-4153-b8b1-90f035c2113f",
   "metadata": {},
   "source": [
    "## Plot the log of the running average using 2D heat map to better visualize noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "6836af45-ca6c-4d31-b625-06212e102805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the running average as a 2D histogram with 1D histograms on the margins\n",
    "\n",
    "bins = {\"x\":141, \"y\": 141}\n",
    "xrange = {\"min\" : min_event, \"max\" : max_event}\n",
    "yrange = {\"min\" : -6, \"max\" : 2}\n",
    "fig, ax, twodimdata = tdh.make_plot(xarr, logrunavg, \"x\", \"y\",xrange ,yrange, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcc7ee-060e-4ef7-b9bd-b3fdfd77e4d6",
   "metadata": {},
   "source": [
    "## Visually estimate the mean of the Gaussian background noise \n",
    "\n",
    "\n",
    "#### Vary the mean value and plot to check if the estimated mean is well aligned with the peak of the noise. The data on the right-half (blue/purple) estimates the right half of the Gaussian noise. The left-most bin of the right-half data is the estimated mean. When the estimated mean is well aligned with the peak, then you may continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "43ee0bf9-aeec-493d-80a5-7cae1bd24a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### estimate the half gaussian fit on the top half of the data.\n",
    "# set the threshold based on visual inspection. here, threshold ~ mu\n",
    "noise_mean = -0.146\n",
    "\n",
    "# compress the filtered data to remove the masked values\n",
    "filtered_logrunavg = ma.masked_less(logrunavg, noise_mean).compressed()\n",
    "\n",
    "# Set all masked values to zero\n",
    "counts_logrunavg, bins_logrunavg, _ = plt.hist(logrunavg, bins = bins['y'], alpha=0.6, color='r')\n",
    "plt.hist(filtered_logrunavg, bins = bins_logrunavg, alpha=0.6, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "afeac049-a791-4845-b63b-394cdbdd0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with halfnorm. visualize for best fit testing. get mu and std dev. consider finding max and setting location as mean\n",
    "mu, std = halfnorm.fit(filtered_logrunavg)\n",
    "\n",
    "# counts_logrunavg, bins_logrunavg, _ = plt.hist(logrunavg, bins = bins['y'], density= True, alpha=0.6, color='r')\n",
    "plt.hist(filtered_logrunavg, bins = bins['y'], density = True, alpha=0.6, color='r')\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = halfnorm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Fit Values: {:.2f} and {:.2f}\".format(mu, std)\n",
    "plt.title(title)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbfa67-9287-4eef-933f-cf936e8dde57",
   "metadata": {},
   "source": [
    "## Using the halfnorm fit on top half of background noise, define 4sigma threshold to isolate the dynamics data and then plot for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "30c2ab99-bc54-4a48-9415-d958b6a41db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using std from halfnorm fit, set the threshold for filtering out noise. Then, filter out noise. Raise 10 to threshold later for burst selection\n",
    "threshold_value = mu - 4*std\n",
    "filtered_values = ma.masked_greater(logrunavg, threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fb557882-5c02-4f69-a58f-1a4a98742716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the log running average and the threshold values\n",
    "plt.plot(xarr, logrunavg, label='Running Average', linestyle='None', marker = 'o', markersize = 5)\n",
    "plt.plot(xarr, filtered_values, label='Threshold Values', linestyle='None', marker = '.', markersize = 5)\n",
    "plt.xlabel('Photon Event #')\n",
    "plt.ylabel('log(Photon Interval Time)')\n",
    "plt.legend()\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09a742-61c9-41be-b280-e90ab34602ec",
   "metadata": {},
   "source": [
    "## Extract the photon data that meets the threshold condition and create a burst index for further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "921c30b5-1b0e-451e-a9d4-4268923e7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define function for extracting the unmasked segments from the thresholded data.\n",
    "def extract_unmasked_indices(masked_array):\n",
    "    unmasked_indices_lists = []\n",
    "    current_indices = []\n",
    "\n",
    "    # iterate through masked array and collect unmasked index segments\n",
    "    for i, value in enumerate(masked_array):\n",
    "        if np.ma.is_masked(value):\n",
    "            if current_indices:\n",
    "                unmasked_indices_lists.append(current_indices)\n",
    "                current_indices = []\n",
    "        else:\n",
    "            current_indices.append(i)\n",
    "\n",
    "    # handle the last segment\n",
    "    if current_indices:\n",
    "        unmasked_indices_lists.append(current_indices)\n",
    "\n",
    "    return unmasked_indices_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "87a38c84-e5dc-47aa-9723-0a019de722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a burst index. Each list is a burst, and each list contains the indices of \n",
    "### the photon events in the original data.\n",
    "burst_index = extract_unmasked_indices(filtered_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ada07-0bca-45b7-ac8a-8ec53569ba34",
   "metadata": {},
   "source": [
    "## Create bi4_bur dataframe and calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6abdb7c7-406a-4b23-84d3-ab6b5c09ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### create bi4_bur dataframe ###\n",
    "################################\n",
    "\n",
    "# prepare empty dataframes to insert calculated burst values\n",
    "bi4_bur_df = pd.DataFrame()\n",
    "bg4_df = pd.DataFrame()\n",
    "\n",
    "bg4_channel_2_photons_total = list()\n",
    "bg4_channel_0_photons_total = list()\n",
    "\n",
    "### calculate each burst record and store in df\n",
    "for burst in burst_index:\n",
    "\n",
    "    # filter out bursts with one or less photons.\n",
    "    if len(burst) <= min_photon_count:\n",
    "        continue\n",
    "\n",
    "    ############################# .bur calculations\n",
    "    \n",
    "    #############################\n",
    "    ### First and Last Photon ###\n",
    "    #############################\n",
    "    \n",
    "    first_photon = burst[0]\n",
    "    last_photon = burst[-1]\n",
    "\n",
    "    ##########################\n",
    "    ### Duration (ms) calc ###\n",
    "    ##########################\n",
    "    \n",
    "    lp_time = all_macro_times[last_photon]*macro_res + all_micro_times[last_photon]*micro_res\n",
    "    fp_time = all_macro_times[first_photon]*macro_res + all_micro_times[first_photon]*micro_res\n",
    "    lp_time_ms = lp_time*1000\n",
    "    fp_time_ms = fp_time*1000\n",
    "    duration = (lp_time_ms - fp_time_ms)\n",
    "\n",
    "    #################################\n",
    "    ### Mean Macro Time (ms) Calc ###\n",
    "    #################################\n",
    "    \n",
    "    # get all the macro times corresponding to the photons in this burst\n",
    "    macro_times = all_macro_times[burst[0]]*macro_res*1000\n",
    "    \n",
    "    # calculate the mean\n",
    "    mean_macro_time = np.mean(macro_times)\n",
    "\n",
    "    ##############################\n",
    "    ### Number of Photons calc ###\n",
    "    ##############################\n",
    "    num_photons = len(burst)\n",
    "\n",
    "    #######################\n",
    "    ### Count Rate calc ###\n",
    "    #######################\n",
    "    count_rate = num_photons / duration\n",
    "\n",
    "    #################################\n",
    "    ### Duration (green)(ms) calc ###\n",
    "    #################################\n",
    "    \n",
    "    # Assuming you have your list of indexes\n",
    "    list_of_indexes = burst\n",
    "    \n",
    "    # Create boolean masks for channels 0 and 2 to select the green channels\n",
    "    mask_channel_0 = routing_channels[list_of_indexes] == 0\n",
    "    mask_channel_2 = routing_channels[list_of_indexes] == 2\n",
    "    \n",
    "    # Use boolean masks to filter the indexes\n",
    "    indexes_channel_0 = np.array(list_of_indexes)[mask_channel_0]\n",
    "    indexes_channel_2 = np.array(list_of_indexes)[mask_channel_2]\n",
    "    \n",
    "    # Output the filtered indexes\n",
    "    # print(\"Indexes corresponding to channel 0:\", indexes_channel_0)\n",
    "    # print(\"Indexes corresponding to channel 2:\", indexes_channel_2)\n",
    "    \n",
    "    # Find the minimum and maximum indexes across both resulting arrays. These are first and last green \n",
    "    # photon in this burst. handle situations where only one channel has photons or none have photons.\n",
    "    if len(indexes_channel_0) > 0 and len(indexes_channel_2) > 0:\n",
    "        first_green_photon = min(np.min(indexes_channel_0), np.min(indexes_channel_2))\n",
    "        last_green_photon = max(np.max(indexes_channel_0), np.max(indexes_channel_2))\n",
    "    elif len(indexes_channel_0) >= 2:\n",
    "        first_green_photon = np.min(indexes_channel_0)\n",
    "        last_green_photon = np.max(indexes_channel_0)\n",
    "    elif len(indexes_channel_2) >= 2:\n",
    "        first_green_photon = np.min(indexes_channel_2)\n",
    "        last_green_photon = np.max(indexes_channel_2)\n",
    "    else:\n",
    "        first_green_photon = None\n",
    "        last_green_photon = None\n",
    "\n",
    "    # Calculate duration or set as NaN\n",
    "    if first_green_photon != None and last_green_photon != None:\n",
    "        lgp_time = all_macro_times[last_green_photon]*macro_res + all_micro_times[last_green_photon]*micro_res\n",
    "        fgp_time = all_macro_times[first_green_photon]*macro_res + all_micro_times[first_green_photon]*micro_res\n",
    "        lgp_time_ms = lgp_time*1000\n",
    "        fgp_time_ms = fgp_time*1000\n",
    "        duration_green = (lgp_time_ms - fgp_time_ms)\n",
    "    else:\n",
    "        duration_green = np.nan\n",
    "\n",
    "    #########################################\n",
    "    #### Mean Macro Time (green)(ms) calc ###\n",
    "    #########################################\n",
    "    \n",
    "    # get all the macro times corresponding to the photons in this burst\n",
    "    macro_times_ch0 = all_macro_times[indexes_channel_0]*macro_res*1000\n",
    "    macro_times_ch2 = all_macro_times[indexes_channel_2]*macro_res*1000\n",
    "    \n",
    "    # Concatenate the arrays along the appropriate axis\n",
    "    combined_macro_times = np.concatenate([macro_times_ch0, macro_times_ch2], axis=0)\n",
    "    \n",
    "    # calculate the mean\n",
    "    mean_macro_time_green = np.mean(combined_macro_times, axis=0)\n",
    "\n",
    "    #################################\n",
    "    ### Number of Photons (green) ###\n",
    "    #################################\n",
    "    num_photons_gr = len(indexes_channel_0) + len(indexes_channel_2)\n",
    "\n",
    "    ########################\n",
    "    ### Green Count Rate ###\n",
    "    ########################\n",
    "    count_rate_gr = num_photons_gr / duration_green\n",
    "\n",
    "    bur_new_row = {'First Photon': [first_photon],'Last Photon': [last_photon],\n",
    "               'Duration (ms)': [duration],'Mean Macro Time (ms)': [mean_macro_time],\n",
    "               'Number of Photons': [num_photons], 'Count Rate (kHz)': [count_rate],\n",
    "               'Duration (green) (ms)': [duration_green], \n",
    "               'Mean Macro Time (green) (ms)': [mean_macro_time_green],\n",
    "               'Number of Photons (green)': [num_photons_gr],\n",
    "               'Green Count Rate (kHz)': count_rate_gr\n",
    "              }\n",
    "    \n",
    "    bur_new_record = pd.DataFrame.from_dict(bur_new_row)\n",
    "    \n",
    "    ### append record to df\n",
    "    bi4_bur_df = pd.concat([bi4_bur_df, bur_new_record], ignore_index=True)\n",
    "\n",
    "    ######################## .bg4 calculations\n",
    "\n",
    "    ################\n",
    "    ### Ng-p-all ###\n",
    "    ################\n",
    "\n",
    "    # Find the indices in burst where the corresponding channel is 2 and 0 < micro time < 12499 using list comprehension\n",
    "    bg4_channel_2_photons = [index for index in burst if routing_channels[index] == 2 and \n",
    "                                 bg4_micro_time_min < all_micro_times[index] < bg4_micro_time_max]\n",
    "    bg4_channel_2_count = len(bg4_channel_2_photons)\n",
    "\n",
    "    # append bg4 chn 2 photons to total list for MLE estimations later\n",
    "    bg4_channel_2_photons_total.extend(bg4_channel_2_photons)\n",
    "\n",
    "\n",
    "    ################\n",
    "    ### Ng-s-all ###\n",
    "    ################\n",
    "\n",
    "    # Find the indices in burst where the corresponding channel is 0 using list comprehension\n",
    "    bg4_channel_0_photons = [index for index in burst if routing_channels[index] == 0 and \n",
    "                                 bg4_micro_time_min < all_micro_times[index] < bg4_micro_time_max]\n",
    "    bg4_channel_0_count = len(bg4_channel_0_photons)\n",
    "\n",
    "    # append bg4 chn 0 photons to total list for MLE estimations later\n",
    "    bg4_channel_0_photons_total.extend(bg4_channel_0_photons)\n",
    "\n",
    "    ##############################################\n",
    "    ### Number of Photons (fit window) (green) ###\n",
    "    ##############################################\n",
    "\n",
    "    bg4_total_count = bg4_channel_2_count + bg4_channel_0_count\n",
    "\n",
    "    ##############################\n",
    "    ### r Experimental (green) ###\n",
    "    ##############################\n",
    "\n",
    "    bg4_rexp = calc_flour_aniso(g_factor, bg4_channel_2_count, bg4_channel_0_count, l1_japan_corr, l2_japan_corr)\n",
    "\n",
    "    #########################\n",
    "    ### r Scatter (green) ###\n",
    "    #########################\n",
    "\n",
    "    bg4_rscat = calc_flour_aniso(g_factor, bg4_channel_2_count - bg4_bkg_para, bg4_channel_0_count - bg4_bkg_perp, l1_japan_corr, l2_japan_corr)\n",
    "\n",
    "    ### create and store new record for this burst \n",
    "\n",
    "    bg4_new_row = {'Ng-p-all': [bg4_channel_2_count],\n",
    "                   'Ng-s-all': [bg4_channel_0_count],\n",
    "                   'Number of Photons (fit window) (green)' : [bg4_total_count],\n",
    "                   'r Scatter (green)' : [bg4_rscat],\n",
    "                   'r Experimental (green)' : [bg4_rexp]\n",
    "                  }\n",
    "    bg4_new_record = pd.DataFrame.from_dict(bg4_new_row)\n",
    "    \n",
    "    ### append record to df\n",
    "    bg4_df = pd.concat([bg4_df, bg4_new_record], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac8c70-9e08-4fb1-8db1-573ffb1577fa",
   "metadata": {},
   "source": [
    "## Save the bi4_bur, bg4, br4 and by4 to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d93c720e-9138-4dc5-917e-d442828970c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put output directory at the start\n",
    "\n",
    "output_directory = r'C:\\Users\\2administrator\\Documents\\source\\repos\\feda_tools\\tests\\burstid_selection_viz_tool'\n",
    "bur_filename = os.path.splitext(os.path.basename(file_p))[0]\n",
    "bur_filepath = os.path.join(output_directory,bur_filename) + \".bur\"\n",
    "bi4_bur_df.to_csv(bur_filepath, sep='\\t', index=False, float_format='%.6f')  # Save without index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ee6ca65b-9a59-43d3-984e-718ffb74027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bur_filename = os.path.splitext(os.path.basename(file_p))[0]\n",
    "bur_filepath = os.path.join(output_directory,bur_filename) + \".bg4\"\n",
    "bg4_df.to_csv(bur_filepath, sep='\\t', index=False, float_format='%.6f')  # Save without index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad8b587-293b-4c10-b0c2-c6e852a12d05",
   "metadata": {},
   "source": [
    "## Mean Micro Time Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a52e01b8-ac80-4d24-b0d4-eec4b7bc6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean micro time for channels 0 and 2 photons: 2123.705680930632\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the arrays: bg4_channel_0_photons, bg4_channel_2_photons, and all_micro_times\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons_total]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "# Combine micro times for both channels\n",
    "combined_micro_times = np.concatenate([micro_times_channel_0, micro_times_channel_2])\n",
    "\n",
    "# Calculate the mean micro time for combined channels\n",
    "mean_micro_time_combined = np.mean(combined_micro_times)\n",
    "\n",
    "# Output the combined mean micro time\n",
    "print(\"Combined mean micro time for channels 0 and 2 photons:\", mean_micro_time_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "f37ac5d6-598d-449a-bd42-d83bae106fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean micro time for channels 0 and 2 photons: 2132.554220005788\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the arrays: bg4_channel_0_photons, bg4_channel_2_photons, and all_micro_times\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons_total]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "# Calculate the mean micro time for channel 0 photons\n",
    "mean_micro_time_channel_0 = np.mean(micro_times_channel_0)\n",
    "\n",
    "# Calculate the mean micro time for channel 2 photons\n",
    "mean_micro_time_channel_2 = np.mean(micro_times_channel_2)\n",
    "\n",
    "# Calculate the mean of the means\n",
    "combined_mean_micro_time = np.mean([mean_micro_time_channel_0, mean_micro_time_channel_2])\n",
    "\n",
    "# Output the combined mean micro time\n",
    "print(\"Combined mean micro time for channels 0 and 2 photons:\", combined_mean_micro_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c7f1b8-72ab-4e59-bcb1-76d89f742897",
   "metadata": {},
   "source": [
    "## Visualize the micro time decay curves for the entire bg4 data set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "fe151c2c-4a3e-416c-a5b6-77be96dfc1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for each bin: [ 1762. 23612. 18406. 13162.  9729.  7470.  5766.  4504.  3470.  2651.\n",
      "  2137.  1685.  1375.  1124.   886.   756.   581.   500.   392.   342.\n",
      "   295.   248.   219.   195.   142.   138.   136.   145.   127.   111.\n",
      "    85.    99.  1168. 15805. 15998. 12201.  9211.  7208.  5408.  4260.\n",
      "  3390.  2759.  2215.  1733.  1373.  1166.   978.   819.   694.   595.\n",
      "   457.   448.   388.   360.   307.   284.   293.   223.   225.   223.\n",
      "   242.   225.   233.   192.]\n",
      "Bin edges: [3.00000000e+00 3.93531250e+02 7.84062500e+02 1.17459375e+03\n",
      " 1.56512500e+03 1.95565625e+03 2.34618750e+03 2.73671875e+03\n",
      " 3.12725000e+03 3.51778125e+03 3.90831250e+03 4.29884375e+03\n",
      " 4.68937500e+03 5.07990625e+03 5.47043750e+03 5.86096875e+03\n",
      " 6.25150000e+03 6.64203125e+03 7.03256250e+03 7.42309375e+03\n",
      " 7.81362500e+03 8.20415625e+03 8.59468750e+03 8.98521875e+03\n",
      " 9.37575000e+03 9.76628125e+03 1.01568125e+04 1.05473438e+04\n",
      " 1.09378750e+04 1.13284062e+04 1.17189375e+04 1.21094688e+04\n",
      " 1.25000000e+04 1.28905312e+04 1.32810625e+04 1.36715938e+04\n",
      " 1.40621250e+04 1.44526562e+04 1.48431875e+04 1.52337188e+04\n",
      " 1.56242500e+04 1.60147812e+04 1.64053125e+04 1.67958438e+04\n",
      " 1.71863750e+04 1.75769062e+04 1.79674375e+04 1.83579688e+04\n",
      " 1.87485000e+04 1.91390312e+04 1.95295625e+04 1.99200938e+04\n",
      " 2.03106250e+04 2.07011562e+04 2.10916875e+04 2.14822188e+04\n",
      " 2.18727500e+04 2.22632812e+04 2.26538125e+04 2.30443438e+04\n",
      " 2.34348750e+04 2.38254062e+04 2.42159375e+04 2.46064688e+04\n",
      " 2.49970000e+04]\n"
     ]
    }
   ],
   "source": [
    "# This data can be used prior to performing burst by burst mle fitting to determine the initial fitting parameters tau, gamma, r0, and rho.\n",
    "\n",
    "# Extract macro times for channel 2 photons\n",
    "macro_times_channel_2 = all_macro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "# Add 12499 to parallel photon microtimes\n",
    "micro_times_channel_0_shifted = micro_times_channel_0 + 12499\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons_total]\n",
    "\n",
    "chn0_shifted_plus_chn2 = np.concatenate([micro_times_channel_0_shifted, micro_times_channel_2])\n",
    "\n",
    "num_bins = 64\n",
    "# Plot the histogram with 128 bins\n",
    "counts, bins, _ = plt.hist(chn0_shifted_plus_chn2, bins=num_bins)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "# Extract the binned data\n",
    "print(\"Counts for each bin:\", counts)\n",
    "print(\"Bin edges:\", bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c532482-acde-405c-8ff4-f27b0853f4fc",
   "metadata": {},
   "source": [
    "## Estimate tau, gamma, r0, and rho using fit23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f8166ff1-6175-4b97-bea6-0bab217f577e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': array([ 3.09918536e+000,  1.45480842e-311,  3.80000000e-001,\n",
      "        1.00000000e-004, -1.00000000e+000,  0.00000000e+000,\n",
      "       -2.49946238e-001, -2.49946238e-001]), 'fixed': array([0, 1, 1, 0], dtype=int16), 'twoIstar': 2662.683000150888, 'model': array([19216.94147365, 16941.25084528, 14935.04523506, 13166.4154227 ,\n",
      "       11607.22922049, 10232.68413244,  9020.91468733,  7952.64475506,\n",
      "        7010.88091311,  6180.64212495,  5448.72143031,  4803.47585655,\n",
      "        4234.64120887,  3733.16879347,  3291.08147612,  2901.34678652,\n",
      "        2557.7650498 ,  2254.87076566,  1987.84566636,  1752.4420705 ,\n",
      "        1544.91531331,  1361.96417872,  1200.6783855 ,  1058.49229218,\n",
      "         933.14408432,   822.63979487,   725.22158525,   639.33978273,\n",
      "         563.62822907,   496.88254851,   438.04098922,   386.16765788,\n",
      "        4090.48937575,  3606.08767506,  3179.05072096,  2802.58289224,\n",
      "        2470.69692056,  2178.11337183,  1920.17799556,  1692.7877044 ,\n",
      "        1492.32530463,  1315.60195591,  1159.80644503,  1022.46046677,\n",
      "         901.37920047,   794.63655509,   700.53453015,   617.57620486,\n",
      "         544.44192598,   479.96831553,   423.12976449,   373.02211792,\n",
      "         328.84829227,   289.90559576,   255.57455042,   225.30903776,\n",
      "         198.62761145,   175.10583872,   154.36954878,   136.08888068,\n",
      "         119.97303607,   105.76565338,    93.24072977,    82.19902596])}\n"
     ]
    }
   ],
   "source": [
    "# estimate tau, gamma, r0, and rho using fit23\n",
    "\n",
    "\n",
    "\n",
    "# is the width of the bins in time\n",
    "dt = 25000/num_bins/1000\n",
    "\n",
    "# period is pulse duration of the laser (50ns)\n",
    "period = macro_res\n",
    "\n",
    "# irf_np = np.zeros_like(counts)\n",
    "\n",
    "irf_np = np.array([260, 1582, 155, 0, 0, 0, 0, 0, 0, 0,\n",
    "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                   0, 22, 1074, 830, 10, 0, 0, 0, 0, 0,\n",
    "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "                   0, 0, 0, 0], dtype=np.float64)\n",
    "\n",
    "bg = np.zeros_like(irf_np)\n",
    "\n",
    "fit23 = tttrlib.Fit23(\n",
    "    dt=dt,\n",
    "    irf=irf_np,\n",
    "    background=bg,\n",
    "    period=period,\n",
    "    g_factor=g_factor,\n",
    "    l1=l1_japan_corr, l2=l2_japan_corr\n",
    ")\n",
    "\n",
    "# normalize counts\n",
    "norm_counts = counts/np.max(counts)\n",
    "\n",
    "tau, gamma, r0, rho = 2.0, 0.1, 0.38, 1.22\n",
    "x0 = np.array([tau, gamma, r0, rho])\n",
    "fixed = np.array([0, 1, 1, 0])\n",
    "r2 = fit23(data=counts, initial_values=x0, fixed=fixed, include_model = True)\n",
    "print(r2)\n",
    "# output key from DecayFit23.h on tttrlib github.\n",
    "# x[0] fluorescence lifetime - tau;\n",
    "# x[1] fraction of scattered light - gamma;\n",
    "# x[2] fundamental anisotropy - r0\n",
    "# x[3] rotational time - rho;\n",
    "# x[4] softbifl - flag specifying the type of bifl fit (not used here)\n",
    "# x[5] p2s_twoIstar - flag specifying the type of chi2 calculation (not used here)\n",
    "# x[6] background corrected anisotropy\n",
    "# x[7] anisotropy without background correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e797ede-5fd0-4656-b014-03db6d53ab94",
   "metadata": {},
   "source": [
    "## Use MLE data to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "0ddb2554-e3b8-43bb-b45d-ebe5e431ba30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plot fit23 benchmark article online\n",
    "\n",
    "conv_stop = 64\n",
    "param = np.array([r2['x'][0], r2['x'][1], r2['x'][2], r2['x'][3]])\n",
    "# param = np.array([tau, gamma, r0, rho])\n",
    "corrections = np.array([period, g_factor, l1_japan_corr, l2_japan_corr, conv_stop])\n",
    "model = np.zeros_like(irf_np)\n",
    "bg = np.zeros_like(irf_np)\n",
    "tttrlib.DecayFit23.modelf(param, irf_np, bg, dt, corrections, model)\n",
    "n_photons = len(chn0_shifted_plus_chn2) # normalize for some reason. see tttrlib fit23 benchmark article.\n",
    "model *= n_photons / np.sum(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "7cc898f6-cbfd-4f39-9461-95e62b475fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x2ad9c682210>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.semilogy([x for x in fit23.data], label='Data')\n",
    "plt.semilogy([x for x in fit23.irf], label='IRF')\n",
    "plt.semilogy([x for x in fit23.model], label='Model')\n",
    "# Set the y-axis limits\n",
    "plt.ylim(0.001, 100000)\n",
    "# plt.title.set_text(r'Example decay')\n",
    "# plt.set_ylabel(r'Counts')\n",
    "# plt.set_xlabel(r'Channel Nbr.')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "aefe5c1a-fd32-4a9b-896e-8abd47e59724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get MLE to fit mean micro time decays. extract model parameters. use to fit each burst and extract lifetimes from bursts.\n",
    "# how many bursts per second are selected and how many bursts per seconds after filter. before and after thresdhol, what is the burst per second? Single molecule order of mag is 2-3 burst per second byt the time you have lifetimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05370953-eba3-4dda-9e12-090c527ea2be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
