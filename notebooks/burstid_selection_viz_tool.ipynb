{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf0902c-cc14-4ae0-9dc3-0fa7c2491a8d",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3052ae27-0a40-4d08-b145-f84eb89e1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pathlib\n",
    "import tttrlib\n",
    "\n",
    "import os\n",
    "\n",
    "from feda_tools import twodim_hist as tdh\n",
    "from feda_tools import utilities as utils\n",
    "from feda_tools import analysis as an\n",
    "\n",
    "from decimal import Decimal, getcontext\n",
    "\n",
    "import numpy.ma as ma\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import halfnorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e9ad2-aec6-4ec5-908c-00ddb355f94c",
   "metadata": {},
   "source": [
    "## Declare required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "dac192f1-4cf3-4b97-b9ac-9551edd81bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flour_aniso(g_factor, intensity_para, intensity_perp, l1_japan_corr, l2_japan_corr):\n",
    "    ### Fluorescence Anisotropy calculation. see equation 7 in Kudryavtsev, V., Sikor, M., Kalinin, S., Mokranjac, D., Seidel, C.A.M. and Lamb, D.C. (2012), \n",
    "    ### Combining MFD and PIE for Accurate Single-Pair FÃ¶rster Resonance Energy Transfer Measurements. ChemPhysChem, 13: 1060-1078. https://doi.org/10.1002/cphc.201100822\n",
    "    \n",
    "    return (g_factor * intensity_para - intensity_perp) / ((1 - 3 * l2_japan_corr) * g_factor * intensity_para + (2 - 3 * l1_japan_corr) * intensity_perp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15b425-cdea-4f49-9141-974fccc68131",
   "metadata": {},
   "source": [
    "## Load target PTU file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "56416358-d044-42cf-acff-e49b575d19f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PTU Files\n",
    "# file_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30')\n",
    "# bid_path = pathlib.Path('//130.127.188.19/projects/FoxP_FKH-DNA/20220314_FoxP1_data_all_NK/20220310_V78C_monomer_NK/V78C_monomer_FoxP1_1hr/burstwise_All 0.2027#30/BIDs_30ph')\n",
    "\n",
    "### for testing purposes ###\n",
    "# file_path = pathlib.Path('C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu')\n",
    "\n",
    "#total time 816.9 seconds for this file\n",
    "file_p ='C:/Users/2administrator/Documents/source/repos/feda_tools/test data/2022/03_02_22_Troubleshooting_detection_efficiencies/Combined_old_thresholds/Split_After_Adjust_HF_54000s_pinhole6-000000.ptu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c37af5-7223-4a4f-b10e-0eacfb495a03",
   "metadata": {},
   "source": [
    "## Initialize tttrlib data and extract important global data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "0bf7205d-e2f0-42bd-92a1-916a43bf2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tttrlib.TTTR(file_p, 'PTU')\n",
    "all_macro_times = data.macro_times\n",
    "all_micro_times = data.micro_times\n",
    "routing_channels =  data.routing_channels\n",
    "\n",
    "# total duration in seconds\n",
    "total_duration = all_macro_times[-1] * macro_res\n",
    "\n",
    "#in seconds. usually the first plots are in ms to see the bursts.\n",
    "macro_res =data.get_header().macro_time_resolution\n",
    "micro_res = data.get_header().micro_time_resolution\n",
    "\n",
    "min_event = 0\n",
    "max_event = 300000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fe224-8caf-46a3-9a0d-532345adaab1",
   "metadata": {},
   "source": [
    "## Determine analysis settings for bur, bg4, by4, and br4 calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "681c0cfe-13f2-4e14-9e37-a3183b60673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# photon count threshold for burst selection\n",
    "min_photon_count = 60\n",
    "\n",
    "# flourescence anisotropy parameters\n",
    "g_factor = 1.2\n",
    "l1_japan_corr = 0.0308\n",
    "l2_japan_corr = 0.0368\n",
    "# bkg signals required for r Scatter calculations\n",
    "bg4_bkg_sig = 0\n",
    "br4_bkg_sig = 0\n",
    "by4_bkg_sig = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b2f2-15a8-4f05-9bd5-000d6edeaf5b",
   "metadata": {},
   "source": [
    "## Calculate interphoton arrival time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "48e6c51c-a284-49fc-a41f-04b03dc3f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through macro and micro times to calculate delta time between photon events\n",
    "arr_size = len(all_macro_times) - 1\n",
    "photon_time_intervals = np.zeros(arr_size, dtype = np.float64)\n",
    "lw = 0.25\n",
    "for i in range(0, len(photon_time_intervals)):\n",
    "    photon_1 = (all_macro_times[i]*macro_res) + (all_micro_times[i]*micro_res)\n",
    "    photon_2 = (all_macro_times[i+1]*macro_res) + (all_micro_times[i+1]*micro_res)\n",
    "    photon_time_intervals[i] = (photon_2 - photon_1)*1000\n",
    "\n",
    "# create photon ID array\n",
    "photon_ids = np.arange(1, arr_size + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "01fb470b-fc0e-49f6-98e5-6d16f39467f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3073616332567326\n"
     ]
    }
   ],
   "source": [
    "print(photon_time_intervals[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26a8f8-c3e7-4bae-8ffb-d13b35b816b9",
   "metadata": {},
   "source": [
    "## Plot the Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "d38e86b8-8598-443e-83b6-561d1fd0b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot interactive\n",
    "%matplotlib qt\n",
    "\n",
    "# raw data, no running average\n",
    "plt.plot(photon_ids, np.log10(photon_time_intervals), linewidth = 0.25)\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0016ec6-6c36-498d-a6b4-a2e2f1410c95",
   "metadata": {},
   "source": [
    "## Plot the log of the running average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8a02b49a-8a02-49a7-b0f7-84055deb7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_average(data, window_size):\n",
    "    window = np.ones(window_size) / window_size\n",
    "    return np.convolve(data, window, mode='valid')\n",
    "\n",
    "# Create a NumPy array\n",
    "data = photon_time_intervals\n",
    "\n",
    "# Set the window size for the running average\n",
    "window_size = 30\n",
    "\n",
    "# Calculate the running average\n",
    "running_avg = running_average(data, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f4625dea-483e-47b1-9d90-ea31924d5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the running average\n",
    "# plt.plot(data, label='Original Data')\n",
    "lw = 0.25\n",
    "xarr = np.arange(window_size - 1, len(data))\n",
    "logrunavg = np.log10(running_avg)\n",
    "plt.plot(xarr, logrunavg, label='Running Average', linewidth = lw)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96a3ff9-882b-4153-b8b1-90f035c2113f",
   "metadata": {},
   "source": [
    "## Plot the log of the running average using 2D heat map to better visualize noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "6836af45-ca6c-4d31-b625-06212e102805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the running average as a 2D histogram with 1D histograms on the margins\n",
    "\n",
    "bins = {\"x\":141, \"y\": 141}\n",
    "xrange = {\"min\" : min_event, \"max\" : max_event}\n",
    "yrange = {\"min\" : -6, \"max\" : 2}\n",
    "fig, ax, twodimdata = tdh.make_plot(xarr, logrunavg, \"x\", \"y\",xrange ,yrange, bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfcc7ee-060e-4ef7-b9bd-b3fdfd77e4d6",
   "metadata": {},
   "source": [
    "## Visually estimate the mean of the Gaussian background noise \n",
    "\n",
    "\n",
    "#### Vary the mean value and plot to check if the estimated mean is well aligned with the peak of the noise. The data on the right-half (blue/purple) estimates the right half of the Gaussian noise. The left-most bin of the right-half data is the estimated mean. When the estimated mean is well aligned with the peak, then you may continue to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "43ee0bf9-aeec-493d-80a5-7cae1bd24a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "### estimate the half gaussian fit on the top half of the data.\n",
    "# set the threshold based on visual inspection. here, threshold ~ mu\n",
    "noise_mean = -0.146\n",
    "\n",
    "# compress the filtered data to remove the masked values\n",
    "filtered_logrunavg = ma.masked_less(logrunavg, noise_mean).compressed()\n",
    "\n",
    "# Set all masked values to zero\n",
    "counts_logrunavg, bins_logrunavg, _ = plt.hist(logrunavg, bins = bins['y'], alpha=0.6, color='r')\n",
    "plt.hist(filtered_logrunavg, bins = bins_logrunavg, alpha=0.6, color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "afeac049-a791-4845-b63b-394cdbdd0467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with halfnorm. visualize for best fit testing. get mu and std dev\n",
    "mu, std = halfnorm.fit(filtered_logrunavg)\n",
    "\n",
    "# counts_logrunavg, bins_logrunavg, _ = plt.hist(logrunavg, bins = bins['y'], density= True, alpha=0.6, color='r')\n",
    "plt.hist(filtered_logrunavg, bins = bins['y'], density = True, alpha=0.6, color='r')\n",
    "\n",
    "# Plot the PDF.\n",
    "xmin, xmax = plt.xlim()\n",
    "x = np.linspace(xmin, xmax, 100)\n",
    "p = halfnorm.pdf(x, mu, std)\n",
    "\n",
    "plt.plot(x, p, 'k', linewidth=2)\n",
    "title = \"Fit Values: {:.2f} and {:.2f}\".format(mu, std)\n",
    "plt.title(title)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fbfa67-9287-4eef-933f-cf936e8dde57",
   "metadata": {},
   "source": [
    "## Using the halfnorm fit on top half of background noise, define 4sigma threshold to isolate the dynamics data and then plot for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "30c2ab99-bc54-4a48-9415-d958b6a41db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using std from halfnorm fit, set the threshold for filtering out noise. Then, filter out noise. Raise 10 to threshold later for burst selection\n",
    "threshold_value = mu - 4*std\n",
    "filtered_values = ma.masked_greater(logrunavg, threshold_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "fb557882-5c02-4f69-a58f-1a4a98742716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the log running average and the threshold values\n",
    "plt.plot(xarr, logrunavg, label='Running Average', linestyle='None', marker = 'o', markersize = 5)\n",
    "plt.plot(xarr, filtered_values, label='Threshold Values', linestyle='None', marker = '.', markersize = 5)\n",
    "plt.xlabel('Photon Event #')\n",
    "plt.ylabel('log(Photon Interval Time)')\n",
    "plt.legend()\n",
    "plt.xlim(min_event, max_event)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d09a742-61c9-41be-b280-e90ab34602ec",
   "metadata": {},
   "source": [
    "## Extract the photon data that meets the threshold condition and create a burst index for further calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "921c30b5-1b0e-451e-a9d4-4268923e7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### define function for extracting the unmasked segments from the thresholded data.\n",
    "def extract_unmasked_indices(masked_array):\n",
    "    unmasked_indices_lists = []\n",
    "    current_indices = []\n",
    "\n",
    "    # iterate through masked array and collect unmasked index segments\n",
    "    for i, value in enumerate(masked_array):\n",
    "        if np.ma.is_masked(value):\n",
    "            if current_indices:\n",
    "                unmasked_indices_lists.append(current_indices)\n",
    "                current_indices = []\n",
    "        else:\n",
    "            current_indices.append(i)\n",
    "\n",
    "    # handle the last segment\n",
    "    if current_indices:\n",
    "        unmasked_indices_lists.append(current_indices)\n",
    "\n",
    "    return unmasked_indices_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "87a38c84-e5dc-47aa-9723-0a019de722f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get a burst index. Each list is a burst, and each list contains the indices of \n",
    "### the photon events in the original data.\n",
    "burst_index = extract_unmasked_indices(filtered_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ada07-0bca-45b7-ac8a-8ec53569ba34",
   "metadata": {},
   "source": [
    "## Create bi4_bur dataframe and calculate statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "6abdb7c7-406a-4b23-84d3-ab6b5c09ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "### create bi4_bur dataframe ###\n",
    "################################\n",
    "\n",
    "# prepare empty dataframes to insert calculated burst values\n",
    "bi4_bur_df = pd.DataFrame()\n",
    "bg4_df = pd.DataFrame()\n",
    "\n",
    "### calculate each burst record and store in df\n",
    "for burst in burst_index:\n",
    "\n",
    "    # filter out bursts with one or less photons.\n",
    "    if len(burst) <= min_photon_count:\n",
    "        continue\n",
    "\n",
    "    ############################# .bur calculations\n",
    "    \n",
    "    #############################\n",
    "    ### First and Last Photon ###\n",
    "    #############################\n",
    "    \n",
    "    first_photon = burst[0]\n",
    "    last_photon = burst[-1]\n",
    "\n",
    "    ##########################\n",
    "    ### Duration (ms) calc ###\n",
    "    ##########################\n",
    "    \n",
    "    lp_time = all_macro_times[last_photon]*macro_res + all_micro_times[last_photon]*micro_res\n",
    "    fp_time = all_macro_times[first_photon]*macro_res + all_micro_times[first_photon]*micro_res\n",
    "    lp_time_ms = lp_time*1000\n",
    "    fp_time_ms = fp_time*1000\n",
    "    duration = (lp_time_ms - fp_time_ms)\n",
    "\n",
    "    #################################\n",
    "    ### Mean Macro Time (ms) Calc ###\n",
    "    #################################\n",
    "    \n",
    "    # get all the macro times corresponding to the photons in this burst\n",
    "    macro_times = all_macro_times[burst[0]]*macro_res*1000\n",
    "    \n",
    "    # calculate the mean\n",
    "    mean_macro_time = np.mean(macro_times)\n",
    "\n",
    "    ##############################\n",
    "    ### Number of Photons calc ###\n",
    "    ##############################\n",
    "    num_photons = len(burst)\n",
    "\n",
    "    #######################\n",
    "    ### Count Rate calc ###\n",
    "    #######################\n",
    "    count_rate = num_photons / duration\n",
    "\n",
    "    #################################\n",
    "    ### Duration (green)(ms) calc ###\n",
    "    #################################\n",
    "    \n",
    "    # Assuming you have your list of indexes\n",
    "    list_of_indexes = burst\n",
    "    \n",
    "    # Create boolean masks for channels 0 and 2 to select the green channels\n",
    "    mask_channel_0 = routing_channels[list_of_indexes] == 0\n",
    "    mask_channel_2 = routing_channels[list_of_indexes] == 2\n",
    "    \n",
    "    # Use boolean masks to filter the indexes\n",
    "    indexes_channel_0 = np.array(list_of_indexes)[mask_channel_0]\n",
    "    indexes_channel_2 = np.array(list_of_indexes)[mask_channel_2]\n",
    "    \n",
    "    # Output the filtered indexes\n",
    "    # print(\"Indexes corresponding to channel 0:\", indexes_channel_0)\n",
    "    # print(\"Indexes corresponding to channel 2:\", indexes_channel_2)\n",
    "    \n",
    "    # Find the minimum and maximum indexes across both resulting arrays. These are first and last green \n",
    "    # photon in this burst. handle situations where only one channel has photons or none have photons.\n",
    "    if len(indexes_channel_0) > 0 and len(indexes_channel_2) > 0:\n",
    "        first_green_photon = min(np.min(indexes_channel_0), np.min(indexes_channel_2))\n",
    "        last_green_photon = max(np.max(indexes_channel_0), np.max(indexes_channel_2))\n",
    "    elif len(indexes_channel_0) >= 2:\n",
    "        first_green_photon = np.min(indexes_channel_0)\n",
    "        last_green_photon = np.max(indexes_channel_0)\n",
    "    elif len(indexes_channel_2) >= 2:\n",
    "        first_green_photon = np.min(indexes_channel_2)\n",
    "        last_green_photon = np.max(indexes_channel_2)\n",
    "    else:\n",
    "        first_green_photon = None\n",
    "        last_green_photon = None\n",
    "\n",
    "    # Calculate duration or set as NaN\n",
    "    if first_green_photon != None and last_green_photon != None:\n",
    "        lgp_time = all_macro_times[last_green_photon]*macro_res + all_micro_times[last_green_photon]*micro_res\n",
    "        fgp_time = all_macro_times[first_green_photon]*macro_res + all_micro_times[first_green_photon]*micro_res\n",
    "        lgp_time_ms = lgp_time*1000\n",
    "        fgp_time_ms = fgp_time*1000\n",
    "        duration_green = (lgp_time_ms - fgp_time_ms)\n",
    "    else:\n",
    "        duration_green = np.nan\n",
    "\n",
    "    #########################################\n",
    "    #### Mean Macro Time (green)(ms) calc ###\n",
    "    #########################################\n",
    "    \n",
    "    # get all the macro times corresponding to the photons in this burst\n",
    "    macro_times_ch0 = all_macro_times[indexes_channel_0]*macro_res*1000\n",
    "    macro_times_ch2 = all_macro_times[indexes_channel_2]*macro_res*1000\n",
    "    \n",
    "    # Concatenate the arrays along the appropriate axis\n",
    "    combined_macro_times = np.concatenate([macro_times_ch0, macro_times_ch2], axis=0)\n",
    "    \n",
    "    # calculate the mean\n",
    "    mean_macro_time_green = np.mean(combined_macro_times, axis=0)\n",
    "\n",
    "    #################################\n",
    "    ### Number of Photons (green) ###\n",
    "    #################################\n",
    "    num_photons_gr = len(indexes_channel_0) + len(indexes_channel_2)\n",
    "\n",
    "    ########################\n",
    "    ### Green Count Rate ###\n",
    "    ########################\n",
    "    count_rate_gr = num_photons_gr / duration_green\n",
    "\n",
    "    bur_new_row = {'First Photon': [first_photon],'Last Photon': [last_photon],\n",
    "               'Duration (ms)': [duration],'Mean Macro Time (ms)': [mean_macro_time],\n",
    "               'Number of Photons': [num_photons], 'Count Rate (kHz)': [count_rate],\n",
    "               'Duration (green) (ms)': [duration_green], \n",
    "               'Mean Macro Time (green) (ms)': [mean_macro_time_green],\n",
    "               'Number of Photons (green)': [num_photons_gr],\n",
    "               'Green Count Rate (kHz)': count_rate_gr\n",
    "              }\n",
    "    \n",
    "    bur_new_record = pd.DataFrame.from_dict(bur_new_row)\n",
    "    \n",
    "    ### append record to df\n",
    "    bi4_bur_df = pd.concat([bi4_bur_df, bur_new_record], ignore_index=True)\n",
    "\n",
    "    ######################## .bg4 calculations\n",
    "\n",
    "    bg4_micro_time_min = 0\n",
    "    bg4_micro_time_max = 12499\n",
    "\n",
    "    ################\n",
    "    ### Ng-p-all ###\n",
    "    ################\n",
    "\n",
    "    # Find the indices in burst where the corresponding channel is 2 and 0 < micro time < 12499 using list comprehension\n",
    "    bg4_channel_2_photons = [index for index in burst if routing_channels[index] == 2 and \n",
    "                                 bg4_micro_time_min < all_micro_times[index] < bg4_micro_time_max]\n",
    "    bg4_channel_2_count = len(bg4_channel_2_photons)\n",
    "\n",
    "    ################\n",
    "    ### Ng-s-all ###\n",
    "    ################\n",
    "\n",
    "    # Find the indices in burst where the corresponding channel is 0 using list comprehension\n",
    "    bg4_channel_0_photons = [index for index in burst if routing_channels[index] == 0 and \n",
    "                                 bg4_micro_time_min < all_micro_times[index] < bg4_micro_time_max]\n",
    "    bg4_channel_0_count = len(bg4_channel_0_photons)\n",
    "\n",
    "    ##############################################\n",
    "    ### Number of Photons (fit window) (green) ###\n",
    "    ##############################################\n",
    "\n",
    "    bg4_total_count = bg4_channel_2_count + bg4_channel_0_count\n",
    "\n",
    "    ##############################\n",
    "    ### r Experimental (green) ###\n",
    "    ##############################\n",
    "\n",
    "    bg4_rexp = calc_flour_aniso(g_factor, bg4_channel_2_count, bg4_channel_0_count, l1_japan_corr, l2_japan_corr)\n",
    "\n",
    "    #########################\n",
    "    ### r Scatter (green) ###\n",
    "    #########################\n",
    "\n",
    "    bg4_rscat = calc_flour_aniso(g_factor, bg4_channel_2_count - bg4_bkg_sig, bg4_channel_0_count - bg4_bkg_sig, l1_japan_corr, l2_japan_corr)\n",
    "\n",
    "    ### create and store new record for this burst \n",
    "\n",
    "    bg4_new_row = {'Ng-p-all': [bg4_channel_2_count],\n",
    "                   'Ng-s-all': [bg4_channel_0_count],\n",
    "                   'Number of Photons (fit window) (green)' : [bg4_total_count],\n",
    "                   'r Scatter (green)' : [bg4_rscat],\n",
    "                   'r Experimental (green)' : [bg4_rexp]\n",
    "                  }\n",
    "    bg4_new_record = pd.DataFrame.from_dict(bg4_new_row)\n",
    "    \n",
    "    ### append record to df\n",
    "    bg4_df = pd.concat([bg4_df, bg4_new_record], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac8c70-9e08-4fb1-8db1-573ffb1577fa",
   "metadata": {},
   "source": [
    "## Save the bi4_bur, bg4, br4 and by4 to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "d93c720e-9138-4dc5-917e-d442828970c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = r'C:\\Users\\2administrator\\Documents\\source\\repos\\feda_tools\\tests\\burstid_selection_viz_tool'\n",
    "bur_filename = os.path.splitext(os.path.basename(file_p))[0]\n",
    "bur_filepath = os.path.join(output_directory,bur_filename) + \".bur\"\n",
    "bi4_bur_df.to_csv(bur_filepath, sep='\\t', index=False, float_format='%.6f')  # Save without index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "ee6ca65b-9a59-43d3-984e-718ffb74027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = r'C:\\Users\\2administrator\\Documents\\source\\repos\\feda_tools\\tests\\burstid_selection_viz_tool'\n",
    "bur_filename = os.path.splitext(os.path.basename(file_p))[0]\n",
    "bur_filepath = os.path.join(output_directory,bur_filename) + \".bg4\"\n",
    "bg4_df.to_csv(bur_filepath, sep='\\t', index=False, float_format='%.6f')  # Save without index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "a52e01b8-ac80-4d24-b0d4-eec4b7bc6d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean micro time for channels 0 and 2 photons: 2485.477732793522\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the arrays: bg4_channel_0_photons, bg4_channel_2_photons, and all_micro_times\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons]\n",
    "\n",
    "# Combine micro times for both channels\n",
    "combined_micro_times = np.concatenate([micro_times_channel_0, micro_times_channel_2])\n",
    "\n",
    "# Calculate the mean micro time for combined channels\n",
    "mean_micro_time_combined = np.mean(combined_micro_times)\n",
    "\n",
    "# Output the combined mean micro time\n",
    "print(\"Combined mean micro time for channels 0 and 2 photons:\", mean_micro_time_combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "f37ac5d6-598d-449a-bd42-d83bae106fb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined mean micro time for channels 0 and 2 photons: 2488.431868852459\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming you have the arrays: bg4_channel_0_photons, bg4_channel_2_photons, and all_micro_times\n",
    "\n",
    "# Extract micro times for channel 0 photons\n",
    "micro_times_channel_0 = all_micro_times[bg4_channel_0_photons]\n",
    "\n",
    "# Extract micro times for channel 2 photons\n",
    "micro_times_channel_2 = all_micro_times[bg4_channel_2_photons]\n",
    "\n",
    "# Calculate the mean micro time for channel 0 photons\n",
    "mean_micro_time_channel_0 = np.mean(micro_times_channel_0)\n",
    "\n",
    "# Calculate the mean micro time for channel 2 photons\n",
    "mean_micro_time_channel_2 = np.mean(micro_times_channel_2)\n",
    "\n",
    "# Calculate the mean of the means\n",
    "combined_mean_micro_time = np.mean([mean_micro_time_channel_0, mean_micro_time_channel_2])\n",
    "\n",
    "# Output the combined mean micro time\n",
    "print(\"Combined mean micro time for channels 0 and 2 photons:\", combined_mean_micro_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe151c2c-4a3e-416c-a5b6-77be96dfc1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
